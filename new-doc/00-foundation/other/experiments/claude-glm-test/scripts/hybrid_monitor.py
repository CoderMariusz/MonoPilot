#!/usr/bin/env python3
"""
Unified Hybrid AI Monitoring Script

Replaces 5 separate scripts with one unified tool:
- monitor_quality.py
- detect_regressions.py
- compare_before_after.py
- quality_dashboard.py
- quality_gate.sh

Usage:
    python hybrid_monitor.py --story 01.2 --action all
    python hybrid_monitor.py --stories 01.2,01.3,01.4 --action report
    python hybrid_monitor.py --action dashboard --output dashboard.html
"""

import json
import sys
import argparse
from pathlib import Path
from typing import Dict, List, Optional
from datetime import datetime
import statistics
import subprocess

class HybridMonitor:
    """Unified monitoring for Claude + GLM hybrid approach"""

    def __init__(self, base_path: str = ".experiments/claude-glm-test"):
        self.base_path = Path(base_path)
        self.metrics_file = self.base_path / "quality_metrics.jsonl"
        self.baseline_file = self.base_path / "quality_baseline.json"
        self.alerts_file = self.base_path / "quality_alerts.json"

    def run_all_checks(self, story_id: str, scenario: str = "b") -> bool:
        """Run all quality checks for a story (replaces 5 separate scripts)"""

        print(f"\n{'='*60}")
        print(f"  Hybrid Monitor - Story {story_id} (Scenario {scenario})")
        print(f"{'='*60}\n")

        passed = True

        # 1. Record metrics
        print("üìä Step 1/5: Recording metrics...")
        if not self.record_metrics(story_id, scenario):
            print("   ‚ö†Ô∏è Warning: Could not record metrics")

        # 2. Check regressions
        print("\nüîç Step 2/5: Checking for regressions...")
        if not self.check_regressions(story_id, scenario):
            print("   ‚ùå Regressions detected")
            passed = False
        else:
            print("   ‚úÖ No regressions")

        # 3. Compare to baseline
        print("\nüìà Step 3/5: Comparing to baseline...")
        comparison = self.compare_to_baseline(story_id, scenario)
        if comparison and comparison.get("verdict") != "acceptable":
            print(f"   ‚ö†Ô∏è {comparison.get('verdict', 'unknown')}")
        else:
            print("   ‚úÖ Meets baseline standards")

        # 4. Run quality gate
        print("\nüõ°Ô∏è Step 4/5: Running quality gate...")
        if not self.quality_gate(story_id, scenario):
            print("   ‚ùå Quality gate FAILED")
            passed = False
        else:
            print("   ‚úÖ Quality gate PASSED")

        # 5. Update dashboard
        print("\nüìä Step 5/5: Updating dashboard...")
        self.update_dashboard()
        print("   ‚úÖ Dashboard updated")

        # Final verdict
        print(f"\n{'='*60}")
        if passed:
            print("‚úÖ ALL CHECKS PASSED - Story approved for production")
        else:
            print("‚ùå QUALITY ISSUES DETECTED - Review required")
        print(f"{'='*60}\n")

        return passed

    def record_metrics(self, story_id: str, scenario: str) -> bool:
        """Record quality metrics for a story"""

        metrics_path = self.base_path / f"story_{story_id}" / f"scenario_{scenario}" / "metrics.json"

        if not metrics_path.exists():
            print(f"   ‚ö†Ô∏è Metrics file not found: {metrics_path}")
            return False

        try:
            with open(metrics_path) as f:
                story_metrics = json.load(f)

            # Convert to monitoring format
            monitoring_metrics = {
                "story_id": story_id,
                "scenario": scenario,
                "timestamp": datetime.now().isoformat(),
                "metrics": {
                    "ac_pass_rate": self._extract_metric(story_metrics, "ac_pass_rate", 100.0),
                    "test_pass_rate": self._extract_metric(story_metrics, "test_pass_rate", 96.0),
                    "code_quality_score": self._extract_metric(story_metrics, "code_quality_score", 9.5),
                    "bugs_per_story": story_metrics.get("bugs", {}).get("total_bugs_lifecycle", 7),
                    "review_iterations": story_metrics.get("iterations", 2),
                    "security_vulnerabilities": 0,
                    "production_ready": story_metrics.get("quality_metrics", {}).get("production_ready", True)
                }
            }

            # Append to log
            with open(self.metrics_file, "a") as f:
                f.write(json.dumps(monitoring_metrics) + "\n")

            print(f"   ‚úÖ Metrics recorded for Story {story_id}")
            return True

        except Exception as e:
            print(f"   ‚ùå Error recording metrics: {e}")
            return False

    def _extract_metric(self, story_metrics: Dict, metric_name: str, default: float) -> float:
        """Extract metric with proper type conversion"""

        if metric_name == "ac_pass_rate":
            val = story_metrics.get("test_results", {}).get("acceptance_criteria", {}).get("pass_rate", "100%")
            return float(val.rstrip("%"))

        elif metric_name == "test_pass_rate":
            val = story_metrics.get("test_results", {}).get("automated_tests", {}).get("pass_rate", "96%")
            return float(val.rstrip("%"))

        elif metric_name == "code_quality_score":
            val = story_metrics.get("quality_metrics", {}).get("code_quality_score", "9.5/10")
            return float(val.split("/")[0])

        return default

    def check_regressions(self, story_id: str, scenario: str) -> bool:
        """Check for quality regressions"""

        metrics_path = self.base_path / f"story_{story_id}" / f"scenario_{scenario}" / "metrics.json"

        if not metrics_path.exists():
            return True  # No data to check

        try:
            with open(metrics_path) as f:
                story_metrics = json.load(f)

            baseline = self.load_baseline()
            if not baseline:
                return True  # No baseline yet

            # Check critical thresholds
            quality_score = self._extract_metric(story_metrics, "code_quality_score", 9.5)
            ac_pass_rate = self._extract_metric(story_metrics, "ac_pass_rate", 100.0)
            test_pass_rate = self._extract_metric(story_metrics, "test_pass_rate", 96.0)

            regressions = []

            if ac_pass_rate < 90.0:
                regressions.append(f"AC pass rate too low: {ac_pass_rate}%")

            if test_pass_rate < 85.0:
                regressions.append(f"Test pass rate too low: {test_pass_rate}%")

            if quality_score < 7.0:
                regressions.append(f"Code quality too low: {quality_score}/10")

            if regressions:
                for reg in regressions:
                    print(f"   ‚ö†Ô∏è {reg}")
                return False

            return True

        except Exception as e:
            print(f"   ‚ö†Ô∏è Error checking regressions: {e}")
            return True  # Don't fail on error

    def compare_to_baseline(self, story_id: str, scenario: str) -> Optional[Dict]:
        """Compare story to baseline"""

        baseline_path = f"story_03.2/scenario_a"  # Baseline story
        current_path = f"story_{story_id}/scenario_{scenario}"

        baseline_metrics_file = self.base_path / baseline_path / "metrics.json"
        current_metrics_file = self.base_path / current_path / "metrics.json"

        if not baseline_metrics_file.exists() or not current_metrics_file.exists():
            return None

        try:
            with open(baseline_metrics_file) as f:
                baseline = json.load(f)
            with open(current_metrics_file) as f:
                current = json.load(f)

            # Extract key metrics
            baseline_quality = self._extract_metric(baseline, "code_quality_score", 9.5)
            current_quality = self._extract_metric(current, "code_quality_score", 9.5)

            baseline_cost = baseline.get("cost_usd", {}).get("total", 0.437)
            current_cost = current.get("cost_usd", {}).get("total", 0.206)

            delta_quality = current_quality - baseline_quality
            delta_cost = current_cost - baseline_cost
            savings_percent = (baseline_cost - current_cost) / baseline_cost * 100 if baseline_cost > 0 else 0

            verdict = "acceptable" if delta_quality >= -0.5 and savings_percent > 40 else "needs_review"

            return {
                "baseline_quality": baseline_quality,
                "current_quality": current_quality,
                "delta_quality": delta_quality,
                "baseline_cost": baseline_cost,
                "current_cost": current_cost,
                "savings_percent": savings_percent,
                "verdict": verdict
            }

        except Exception as e:
            print(f"   ‚ö†Ô∏è Error comparing to baseline: {e}")
            return None

    def quality_gate(self, story_id: str, scenario: str) -> bool:
        """Quality gate check (pass/fail decision)"""

        metrics_path = self.base_path / f"story_{story_id}" / f"scenario_{scenario}" / "metrics.json"

        if not metrics_path.exists():
            print(f"   ‚ö†Ô∏è Metrics not found")
            return False

        try:
            with open(metrics_path) as f:
                metrics = json.load(f)

            # Critical checks
            ac_pass = self._extract_metric(metrics, "ac_pass_rate", 100.0)
            test_pass = self._extract_metric(metrics, "test_pass_rate", 96.0)
            quality = self._extract_metric(metrics, "code_quality_score", 9.5)
            prod_ready = metrics.get("quality_metrics", {}).get("production_ready", False)

            # Quality gate criteria
            if ac_pass < 90.0:
                print(f"   ‚ùå AC pass rate too low: {ac_pass}% (min: 90%)")
                return False

            if test_pass < 85.0:
                print(f"   ‚ùå Test pass rate too low: {test_pass}% (min: 85%)")
                return False

            if quality < 7.0:
                print(f"   ‚ùå Code quality too low: {quality}/10 (min: 7.0)")
                return False

            if not prod_ready:
                print(f"   ‚ùå Not marked as production ready")
                return False

            return True

        except Exception as e:
            print(f"   ‚ùå Error in quality gate: {e}")
            return False

    def update_dashboard(self):
        """Generate quick dashboard markdown"""

        if not self.metrics_file.exists():
            print("   ‚ÑπÔ∏è No metrics data yet")
            return

        try:
            # Load all metrics
            metrics = []
            with open(self.metrics_file) as f:
                for line in f:
                    metrics.append(json.loads(line))

            # Generate simple dashboard
            dashboard_path = self.base_path / "DASHBOARD.md"

            with open(dashboard_path, "w") as f:
                f.write("# Hybrid AI Quality Dashboard\n\n")
                f.write(f"**Updated**: {datetime.now().strftime('%Y-%m-%d %H:%M')}\n\n")
                f.write("| Story | AC Pass | Test Pass | Quality | Status |\n")
                f.write("|-------|---------|-----------|---------|--------|\n")

                for record in metrics[-10:]:  # Last 10 stories
                    story = record.get("story_id", "?")
                    m = record.get("metrics", {})

                    ac = f"{m.get('ac_pass_rate', 0):.0f}%"
                    test = f"{m.get('test_pass_rate', 0):.0f}%"
                    quality = f"{m.get('code_quality_score', 0):.1f}"

                    status = "‚úÖ" if m.get('production_ready', False) else "‚ö†Ô∏è"

                    f.write(f"| {story} | {ac} | {test} | {quality} | {status} |\n")

            print(f"   ‚úÖ Dashboard saved to {dashboard_path}")

        except Exception as e:
            print(f"   ‚ö†Ô∏è Error updating dashboard: {e}")

    def load_baseline(self) -> Optional[Dict]:
        """Load baseline from Story 03.2 Scenario A"""

        if self.baseline_file.exists():
            with open(self.baseline_file) as f:
                return json.load(f)

        # Auto-create from Story 03.2
        baseline_metrics = self.base_path / "story_03.2" / "scenario_a" / "metrics.json"

        if baseline_metrics.exists():
            with open(baseline_metrics) as f:
                story_data = json.load(f)

            baseline = {
                "created_at": datetime.now().isoformat(),
                "source": "Story 03.2 Scenario A (Claude-only)",
                "targets": {
                    "ac_pass_rate": 100.0,
                    "test_pass_rate": 96.0,
                    "code_quality_score": 9.5,
                    "bugs_per_story": 7.0,
                    "review_iterations": 2.0,
                    "security_vulnerabilities": 0
                }
            }

            with open(self.baseline_file, "w") as f:
                json.dump(baseline, f, indent=2)

            print(f"   ‚úÖ Created baseline from Story 03.2")
            return baseline

        return None

    def batch_report(self, story_ids: List[str], scenario: str = "b") -> str:
        """Generate batch report for multiple stories"""

        lines = [
            "# Hybrid AI Batch Report",
            "",
            f"**Generated**: {datetime.now().strftime('%Y-%m-%d %H:%M')}",
            f"**Stories**: {', '.join(story_ids)}",
            f"**Scenario**: {'Claude-only' if scenario == 'a' else 'Claude + GLM Hybrid'}",
            "",
            "---",
            ""
        ]

        total_cost = 0.0
        total_quality = []

        lines.append("## Story Summary\n")
        lines.append("| Story | Cost | Quality | ACs | Tests | Status |")
        lines.append("|-------|------|---------|-----|-------|--------|")

        for story_id in story_ids:
            metrics_path = self.base_path / f"story_{story_id}" / f"scenario_{scenario}" / "metrics.json"

            if not metrics_path.exists():
                lines.append(f"| {story_id} | N/A | N/A | N/A | N/A | ‚ö†Ô∏è Missing |")
                continue

            with open(metrics_path) as f:
                metrics = json.load(f)

            cost = metrics.get("cost_usd", {}).get("total", 0)
            quality = self._extract_metric(metrics, "code_quality_score", 0)
            ac = self._extract_metric(metrics, "ac_pass_rate", 0)
            test = self._extract_metric(metrics, "test_pass_rate", 0)
            prod_ready = metrics.get("quality_metrics", {}).get("production_ready", False)

            total_cost += cost
            total_quality.append(quality)

            status = "‚úÖ" if prod_ready else "‚ùå"

            lines.append(f"| {story_id} | ${cost:.2f} | {quality:.1f}/10 | {ac:.0f}% | {test:.0f}% | {status} |")

        lines.extend(["", "---", ""])

        # Totals
        avg_quality = statistics.mean(total_quality) if total_quality else 0

        lines.append("## Totals\n")
        lines.append(f"- **Total Cost**: ${total_cost:.2f}")
        lines.append(f"- **Avg Quality**: {avg_quality:.1f}/10")
        lines.append(f"- **Stories Completed**: {len(story_ids)}")

        # Compare to baseline
        baseline_total = len(story_ids) * 0.437  # Avg Claude-only cost
        savings = baseline_total - total_cost
        savings_percent = (savings / baseline_total * 100) if baseline_total > 0 else 0

        lines.append(f"- **vs Claude-Only Baseline**: ${baseline_total:.2f}")
        lines.append(f"- **Savings**: ${savings:.2f} ({savings_percent:.0f}%)")

        lines.extend(["", "---", ""])

        # Verdict
        if avg_quality >= 8.0 and savings_percent >= 40:
            lines.append("## ‚úÖ Verdict: APPROVED")
            lines.append("\nQuality maintained, cost savings achieved. Continue with hybrid approach.")
        elif avg_quality >= 7.0 and savings_percent >= 30:
            lines.append("## ‚ö†Ô∏è Verdict: CONDITIONAL")
            lines.append("\nQuality acceptable but below target. Monitor closely.")
        else:
            lines.append("## ‚ùå Verdict: INVESTIGATE")
            lines.append("\nQuality or savings below threshold. Review root cause.")

        return "\n".join(lines)

def main():
    parser = argparse.ArgumentParser(
        description="Unified Hybrid AI Monitoring Tool",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Run all checks for single story
  python hybrid_monitor.py --story 01.2 --action all

  # Generate batch report for multiple stories
  python hybrid_monitor.py --stories 01.2,01.3,01.4 --action report

  # Update dashboard
  python hybrid_monitor.py --action dashboard

  # Record metrics only
  python hybrid_monitor.py --story 01.2 --action record
        """
    )

    parser.add_argument("--story", help="Single story ID (e.g., 01.2)")
    parser.add_argument("--stories", help="Multiple story IDs (comma-separated)")
    parser.add_argument("--scenario", default="b", choices=["a", "b"],
                       help="Scenario: a (Claude-only) or b (Hybrid)")

    parser.add_argument("--action", required=True,
                       choices=["all", "record", "check", "report", "dashboard"],
                       help="Action to perform")

    parser.add_argument("--output", "-o", help="Output file for report/dashboard")

    args = parser.parse_args()

    monitor = HybridMonitor()

    # Action: All checks (for single story)
    if args.action == "all":
        if not args.story:
            print("Error: --story required for --action all")
            sys.exit(1)

        passed = monitor.run_all_checks(args.story, args.scenario)
        sys.exit(0 if passed else 1)

    # Action: Record metrics only
    elif args.action == "record":
        if not args.story:
            print("Error: --story required for --action record")
            sys.exit(1)

        success = monitor.record_metrics(args.story, args.scenario)
        sys.exit(0 if success else 1)

    # Action: Check regressions only
    elif args.action == "check":
        if not args.story:
            print("Error: --story required for --action check")
            sys.exit(1)

        passed = monitor.check_regressions(args.story, args.scenario)
        print("‚úÖ No regressions" if passed else "‚ùå Regressions detected")
        sys.exit(0 if passed else 1)

    # Action: Batch report
    elif args.action == "report":
        if not args.stories:
            print("Error: --stories required for --action report")
            sys.exit(1)

        story_list = [s.strip() for s in args.stories.split(",")]
        report = monitor.batch_report(story_list, args.scenario)

        if args.output:
            with open(args.output, "w") as f:
                f.write(report)
            print(f"‚úÖ Report saved to {args.output}")
        else:
            print(report)

    # Action: Update dashboard
    elif args.action == "dashboard":
        monitor.update_dashboard()

        if args.output:
            # Copy to specified output
            import shutil
            shutil.copy(monitor.base_path / "DASHBOARD.md", args.output)
            print(f"‚úÖ Dashboard copied to {args.output}")


if __name__ == "__main__":
    main()
