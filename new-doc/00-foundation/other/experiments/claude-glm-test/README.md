# Claude + GLM Multi-Agent Test Framework

Framework do testowania i porÃ³wnywania wydajnoÅ›ci:
- **Scenario A**: Claude Only (peÅ‚ny workflow w Claude)
- **Scenario B**: Claude + GLM (Claude planuje/review, GLM pisze kod)

## ðŸŽ¯ Cel

ZmierzyÄ‡:
1. **OszczÄ™dnoÅ›Ä‡ tokenÃ³w Claude** - czy podziaÅ‚ zadaÅ„ z GLM redukuje uÅ¼ycie Claude?
2. **OszczÄ™dnoÅ›Ä‡ kosztÃ³w** - czy taÅ„szy GLM kompensuje dodatkowÄ… komunikacjÄ™?
3. **JakoÅ›Ä‡ outputu** - czy kod z GLM jest porÃ³wnywalny do Claude?

## ðŸ“ Struktura

```
.experiments/claude-glm-test/
â”œâ”€â”€ test_scenarios/
â”‚   â”œâ”€â”€ scenario_a_claude_only/
â”‚   â”‚   â”œâ”€â”€ input_story.md          # Story do implementacji
â”‚   â”‚   â”œâ”€â”€ context_files/          # Pliki kontekstowe (testy, spec)
â”‚   â”‚   â”œâ”€â”€ output_code.ts          # Kod wygenerowany przez Claude
â”‚   â”‚   â””â”€â”€ metrics.json            # Metryki (tokeny, koszt)
â”‚   â””â”€â”€ scenario_b_claude_glm/
â”‚       â”œâ”€â”€ input_story.md          # To samo story
â”‚       â”œâ”€â”€ context_files/          # Te same pliki kontekstowe
â”‚       â”œâ”€â”€ claude_prompt_for_glm.md # Prompt zaprojektowany przez Claude
â”‚       â”œâ”€â”€ glm_output_code.ts      # Kod wygenerowany przez GLM
â”‚       â”œâ”€â”€ claude_review.md        # Review Claude
â”‚       â””â”€â”€ metrics.json            # Metryki
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ glm_call.py                 # Wrapper na ZhipuAI API
â”‚   â”œâ”€â”€ count_tokens.py             # Licznik tokenÃ³w
â”‚   â””â”€â”€ compare_results.py          # PorÃ³wnanie wynikÃ³w
â”œâ”€â”€ config.json                     # API keys, ustawienia
â””â”€â”€ README.md                       # Ten plik
```

## ðŸ”§ Setup

### 1. Zainstaluj zaleÅ¼noÅ›ci Python

```bash
pip install requests tiktoken
```

### 2. ZdobÄ…dÅº klucz API ZhipuAI

1. Rejestracja: https://open.bigmodel.cn/
2. Darmowe tokeny na start: ~10M tokenÃ³w
3. Dashboard â†’ API Keys â†’ Create Key
4. Wklej do `config.json`:

```json
{
  "zhipu_api_key": "twÃ³j_klucz_tutaj"
}
```

### 3. Przygotuj story do testu

Wybierz story Å›redniej zÅ‚oÅ¼onoÅ›ci z Epic 05 Warehouse (lub innego). Skopiuj:
- Story description â†’ `input_story.md`
- Pliki kontekstowe (testy, UX spec) â†’ `context_files/`

## ðŸ§ª Jak przeprowadziÄ‡ test

### TEST A: Claude Only

1. **PrzekaÅ¼ story Claude** w Antigravity:
   ```
   Zaimplementuj story z .experiments/claude-glm-test/test_scenarios/scenario_a_claude_only/input_story.md

   Kontekst w: context_files/
   Zapisz wynik w: output_code.ts
   ```

2. **Policz tokeny**:
   ```bash
   cd .experiments/claude-glm-test

   # Input tokens (story + kontekst)
   python scripts/count_tokens.py \
     test_scenarios/scenario_a_claude_only/input_story.md \
     test_scenarios/scenario_a_claude_only/context_files/*

   # Output tokens (kod)
   python scripts/count_tokens.py \
     test_scenarios/scenario_a_claude_only/output_code.ts
   ```

3. **Zapisz metryki** w `metrics.json`:
   ```json
   {
     "scenario": "claude_only",
     "total_tokens": 9300,
     "claude_tokens": 9300,
     "glm_tokens": 0,
     "input_tokens": 5800,
     "output_tokens": 3500,
     "cost_usd": 0.0699,
     "iterations": 3,
     "notes": "3 iteracje: initial + 2 poprawki"
   }
   ```

### TEST B: Claude + GLM

1. **Claude projektuje prompt**:
   ```
   Zaprojektuj prompt dla GLM-4-Plus do implementacji story:
   .experiments/claude-glm-test/test_scenarios/scenario_b_claude_glm/input_story.md

   UwzglÄ™dnij kontekst z: context_files/
   Zapisz prompt w: claude_prompt_for_glm.md
   ```

2. **WywoÅ‚aj GLM** (opcja A - rÄ™cznie):
   - Wklej prompt do https://chatglm.cn/
   - Skopiuj odpowiedÅº do `glm_output_code.ts`

   **Lub** (opcja B - przez skrypt):
   ```bash
   python scripts/glm_call.py \
     --prompt "$(cat test_scenarios/scenario_b_claude_glm/claude_prompt_for_glm.md)" \
     --context test_scenarios/scenario_b_claude_glm/context_files/* \
     --model glm-4-plus \
     --output test_scenarios/scenario_b_claude_glm/glm_output_code.ts \
     --json
   ```

3. **Claude robi review**:
   ```
   ZrÃ³b code review dla kodu wygenerowanego przez GLM:
   .experiments/claude-glm-test/test_scenarios/scenario_b_claude_glm/glm_output_code.ts

   WzglÄ™dem spec: input_story.md
   Zapisz review w: claude_review.md
   ```

4. **Zapisz metryki** w `metrics.json`:
   ```json
   {
     "scenario": "claude_glm",
     "total_tokens": 13400,
     "claude_tokens": 2600,
     "glm_tokens": 10800,
     "claude_phases": {
       "planning": 800,
       "review": 1800
     },
     "glm_iterations": 3,
     "cost_usd": 0.0467,
     "notes": "GLM - 3 iteracje poprawek, Claude - tylko planning + review"
   }
   ```

### PorÃ³wnaj wyniki

```bash
python scripts/compare_results.py
```

Output:
```
======================================================================
  SCENARIO COMPARISON: Claude Only vs Claude + GLM
======================================================================

ðŸ“Š SCENARIO A: Claude Only
   Total Tokens:    9,300
   Claude Tokens:   9,300
   Cost (USD):      $0.0699
   Iterations:      3

ðŸ“Š SCENARIO B: Claude + GLM
   Total Tokens:    13,400
   Claude Tokens:   2,600
   GLM Tokens:      10,800
   Cost (USD):      $0.0467
   Iterations:      3

ðŸ’° SAVINGS (Scenario B vs A)
   Claude Tokens:   -6,700 (-72.0%)
   Cost:            -$0.0232 (-33.2%)

ðŸ† WINNER: Claude + GLM
======================================================================
```

## ðŸ“Š PrzykÅ‚adowe metryki (hipotetyczne)

| Metryka | Claude Only | Claude + GLM | Savings |
|---------|-------------|--------------|---------|
| Claude Tokens | 9,300 | 2,600 | **-72%** |
| Total Tokens | 9,300 | 13,400 | +44% |
| Cost (USD) | $0.0699 | $0.0467 | **-33%** |
| Time | 5 min | 7 min | -40% |

## ðŸŽ“ Wnioski z testÃ³w

Po przeprowadzeniu testÃ³w, udokumentuj:

1. **Token Efficiency**: Czy podziaÅ‚ z GLM faktycznie redukuje tokeny Claude?
2. **Cost Savings**: Czy niÅ¼sza cena GLM kompensuje wiÄ™cej total tokenÃ³w?
3. **Quality**: Czy kod z GLM wymaga wiÄ™cej poprawek Claude?
4. **Speed**: KtÃ³ry scenariusz jest szybszy end-to-end?

## ðŸš€ NastÄ™pne kroki

JeÅ›li test wyjdzie pozytywnie:

1. **Automatyzacja**: Skrypt orkiestratora Å‚Ä…czÄ…cy Claude + GLM
2. **Integracja z MonoPilot**: Dodanie GLM jako executor do 7-phase workflow
3. **Optymalizacja**: Fine-tuning podziaÅ‚u zadaÅ„ miÄ™dzy modele
4. **Scaling**: Test na wiÄ™cej story z rÃ³Å¼nych epics

## ðŸ“ Notatki

- GLM-4-Plus: Najlepszy balans jakoÅ›ci/ceny
- GLM-4-Long: UÅ¼yj gdy kontekst > 100K tokenÃ³w
- GLM-4-Flash: Dla prostych taskÃ³w (generowanie testÃ³w?)
- Claude: Zawsze do planowania, architectural decisions, QA

## ðŸ”— Linki

- ZhipuAI Docs: https://open.bigmodel.cn/dev/api
- GLM Pricing: https://open.bigmodel.cn/pricing
- Model Comparison: https://open.bigmodel.cn/models
