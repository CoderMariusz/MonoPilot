# 06.39 - Quality Analytics

**Priority**: P1 (Post-MVP)
**Story Points**: L (Large)
**Type**: fullstack (advanced analytics)
**Phase**: 4 (Analytics & Reporting)
**Model**: OPUS

**State:** ready
**Estimate:** L (4-5 days)
**Primary PRD:** `docs/1-BASELINE/product/modules/quality.md` (FR-QA-022, Section 10 Phase 4)
**Architecture:** `docs/1-BASELINE/architecture/modules/quality.md` (analytics patterns)

---

## Goal

Implement advanced quality analytics including Defect Pareto analysis, NCR/Test Result trending, Statistical Process Control (SPC) charts (X-bar, R-chart), root cause frequency analysis, supplier quality trends, and CAPA effectiveness metrics. Enable QA Managers to identify quality patterns, predict issues, and drive continuous improvement with data-driven insights.

---

## Food Safety Compliance

This story supports **continuous improvement and compliance**:

- [x] **ISO 9001:2015** - Clause 9.1.3 requires analysis and evaluation of quality data
- [x] **HACCP** - Trend analysis for CCP monitoring and verification
- [x] **Six Sigma** - Statistical tools for process control
- [x] **Continuous Improvement** - Data-driven decision making

**Regulatory Context:**
- Quality analytics demonstrate commitment to continuous improvement
- SPC charts identify process capability and stability
- Trend analysis predicts potential quality issues before they occur
- Root cause analysis focuses corrective actions on high-impact areas

---

## MVP Scope

This story implements Phase 4 (Analytics & Reporting) functionality. Quality Analytics extends the basic dashboard (06.37) with advanced statistical analysis and predictive insights.

**MVP Includes**:
- Defect Pareto analysis (top 10 defect types by frequency, 80/20 rule)
- NCR trend analysis (time series with moving average, seasonal patterns)
- Test result trending (parameter-specific trends over time)
- CCP deviation trend analysis (frequency and severity over time)
- Statistical Process Control (SPC) charts (X-bar chart, R-chart for test results)
- Root cause frequency analysis (top root causes from NCR data)
- Supplier quality trends (scorecard trends over time)
- CAPA effectiveness metrics (% effective vs ineffective, time to close)
- Analytics export (CSV, Excel with multiple sheets)
- Scheduled reports (daily/weekly/monthly email delivery)

**Deferred to Phase 5**: Predictive analytics (ML-based failure prediction), correlation analysis (defect type vs supplier/product), advanced Six Sigma tools (Cpk, Ppk).

---

## User Story

As a **QA Manager**, I want to **analyze quality data with statistical tools and trend charts** so that **I can identify patterns, predict issues, and prioritize improvement initiatives**.

As a **Quality Director**, I want to **view CAPA effectiveness and supplier quality trends** so that **I can assess the impact of improvement actions and drive supplier performance**.

---

## Scope

**In scope (this story)**
- Defect Pareto analysis (top 10 defect types, cumulative %)
- NCR trend analysis (time series, moving average)
- Test result trending (parameter-specific, control limits)
- CCP deviation trend analysis (frequency, severity)
- SPC charts: X-bar chart (mean of means), R-chart (range)
- Root cause frequency analysis (top root causes from NCR)
- Supplier quality trends (scorecard over time)
- CAPA effectiveness metrics (% effective, time to close)
- Analytics export (CSV, Excel multi-sheet)
- Scheduled reports (email delivery: daily/weekly/monthly)
- GET /api/quality/analytics/defect-pareto
- GET /api/quality/analytics/ncr-trend
- GET /api/quality/analytics/test-result-trend
- GET /api/quality/analytics/ccp-deviation-trend
- GET /api/quality/analytics/spc-chart
- GET /api/quality/analytics/root-cause-frequency
- GET /api/quality/analytics/supplier-quality-trend
- GET /api/quality/analytics/capa-effectiveness
- POST /api/quality/analytics/export
- POST /api/quality/analytics/schedule-report

**Out of scope (this story)**
- Predictive analytics (ML-based failure prediction) - Phase 5
- Correlation analysis (defect type vs supplier/product) - Phase 5
- Advanced Six Sigma tools (Cpk, Ppk, process capability) - Phase 5
- Real-time anomaly detection (ML-based) - Phase 6
- Custom report builder (drag-and-drop) - Phase 6

---

## Dependencies

### Cross-Epic Dependencies

| Dependency | Story/Epic | Type | What It Provides | Status |
|------------|------------|------|------------------|--------|
| 01.1 | Org Context + RLS | HARD | organizations, users tables | Ready |

### Within Epic Dependencies

| Dependency | Story | Type | What It Provides |
|------------|-------|------|------------------|
| 06.5 | Incoming Inspection | HARD | quality_inspections data |
| 06.6 | Test Results Recording | HARD | test_results data for trending |
| 06.9 | Basic NCR Creation | HARD | ncr_reports data |
| 06.13 | NCR Workflow | HARD | NCR root cause data |
| 06.22 | CCP Definition | HARD | CCP data |
| 06.23 | CCP Monitoring Desktop | HARD | CCP monitoring records |
| 06.25 | CCP Deviation Handling | HARD | CCP deviation data |
| 06.31 | CAPA Creation | HARD | CAPA data |
| 06.32 | CAPA Action Items | HARD | CAPA action completion data |
| 06.33 | CAPA Effectiveness Check | HARD | CAPA effectiveness data |
| 06.34 | Supplier Quality Ratings | HARD | Supplier scorecard data |
| 06.37 | Quality Dashboard | SOFT | Dashboard framework for reuse |

### Provides To (Downstream)

| Story | What This Provides |
|-------|-------------------|
| Phase 5 | Predictive Analytics - Base analytics for ML models |
| Phase 6 | Custom Report Builder - Analytics components for reuse |

---

## Database Migration

**No new tables required**. Analytics reads from existing tables with optimized queries and aggregations.

**Optional: Materialized Views for Performance**

```sql
-- Migration: YYYYMMDDHHMMSS_create_quality_analytics_mvs.sql

-- =============================================================================
-- Materialized View: Defect Pareto (refreshed hourly)
-- =============================================================================

CREATE MATERIALIZED VIEW quality_defect_pareto_mv AS
SELECT
    org_id,
    defect_type,
    severity,
    COUNT(*) AS defect_count,
    ROUND(100.0 * COUNT(*) / SUM(COUNT(*)) OVER (PARTITION BY org_id), 2) AS percentage,
    ROUND(SUM(100.0 * COUNT(*) / SUM(COUNT(*)) OVER (PARTITION BY org_id)) OVER (PARTITION BY org_id ORDER BY COUNT(*) DESC), 2) AS cumulative_percentage
FROM quality_test_results
WHERE status = 'fail'
  AND created_at > NOW() - INTERVAL '90 days'
GROUP BY org_id, defect_type, severity;

CREATE UNIQUE INDEX idx_defect_pareto_org_defect ON quality_defect_pareto_mv(org_id, defect_type);

-- =============================================================================
-- Materialized View: Root Cause Frequency (refreshed hourly)
-- =============================================================================

CREATE MATERIALIZED VIEW quality_root_cause_frequency_mv AS
SELECT
    org_id,
    root_cause_category,
    COUNT(*) AS ncr_count,
    ROUND(100.0 * COUNT(*) / SUM(COUNT(*)) OVER (PARTITION BY org_id), 2) AS percentage
FROM ncr_reports
WHERE root_cause_category IS NOT NULL
  AND created_at > NOW() - INTERVAL '90 days'
GROUP BY org_id, root_cause_category;

CREATE UNIQUE INDEX idx_root_cause_org_category ON quality_root_cause_frequency_mv(org_id, root_cause_category);

-- =============================================================================
-- Refresh Function (scheduled via cron)
-- =============================================================================

CREATE OR REPLACE FUNCTION refresh_quality_analytics_mvs()
RETURNS void AS $$
BEGIN
    REFRESH MATERIALIZED VIEW CONCURRENTLY quality_defect_pareto_mv;
    REFRESH MATERIALIZED VIEW CONCURRENTLY quality_root_cause_frequency_mv;
END;
$$ LANGUAGE plpgsql;

-- Schedule: Every hour
-- SELECT cron.schedule('refresh-analytics', '0 * * * *', 'SELECT refresh_quality_analytics_mvs();');

-- =============================================================================
-- RLS Policies
-- =============================================================================

ALTER MATERIALIZED VIEW quality_defect_pareto_mv SET (security_barrier = true);
ALTER MATERIALIZED VIEW quality_root_cause_frequency_mv SET (security_barrier = true);

CREATE POLICY "defect_pareto_org" ON quality_defect_pareto_mv
    FOR SELECT USING (org_id = (SELECT org_id FROM users WHERE id = auth.uid()));

CREATE POLICY "root_cause_org" ON quality_root_cause_frequency_mv
    FOR SELECT USING (org_id = (SELECT org_id FROM users WHERE id = auth.uid()));
```

---

## Acceptance Criteria (Given/When/Then)

### AC-1: Defect Pareto Analysis

```gherkin
Scenario: Display defect Pareto chart
  Given test result data with defects
  When user navigates to Quality > Analytics > Defect Pareto
  Then Pareto chart displays:
    - X-axis: Defect type (top 10, sorted by frequency descending)
    - Y-axis (left): Defect count (bars)
    - Y-axis (right): Cumulative percentage (line)
    - 80% rule line (horizontal at 80%)
    - Bars color-coded by severity (critical=red, major=orange, minor=yellow)
  And data table below chart shows:
    | Defect Type | Count | % | Cumulative % |
  And chart updates when date filter changes

Scenario: Defect Pareto drill-down
  Given user clicks on defect type bar
  Then navigates to /quality/test-results?defect_type={type}&status=fail
  And shows all failed test results with that defect type
```

### AC-2: NCR Trend Analysis

```gherkin
Scenario: Display NCR trend with moving average
  Given NCR data for last 90 days
  When user views NCR Trend chart
  Then line chart displays:
    - X-axis: Date (daily/weekly buckets based on range)
    - Y-axis: NCR count
    - Primary line: Daily NCR count (blue)
    - Secondary line: 7-day moving average (orange)
    - Data points hoverable with tooltip
  And trend annotation shows: "↑ 15% increase vs previous period"

Scenario: NCR trend by severity
  Given NCR data with severity levels
  When user toggles "Show by Severity"
  Then chart displays:
    - Stacked area chart
    - Critical (red), Major (orange), Minor (yellow)
    - Tooltip shows breakdown by severity
```

### AC-3: Test Result Trending

```gherkin
Scenario: Display test result trend for parameter
  Given test results for parameter "Moisture %" over time
  When user selects parameter from dropdown
  Then line chart displays:
    - X-axis: Date (batch completion date)
    - Y-axis: Test result value
    - Primary line: Actual test values (blue)
    - Upper control limit (UCL) line (red dashed)
    - Lower control limit (LCL) line (red dashed)
    - Target value line (green dashed)
    - Specification min/max lines (gray dashed)
  And out-of-spec points highlighted in red
  And trend line overlay (optional, linear regression)

Scenario: Test result trend statistics
  Given test result trend displayed
  Then statistics panel shows:
    | Metric | Value |
    | Mean | 12.5% |
    | Std Dev | 0.8% |
    | Min | 10.2% |
    | Max | 14.1% |
    | Out of Spec | 3 (2.5%) |
```

### AC-4: CCP Deviation Trend Analysis

```gherkin
Scenario: Display CCP deviation frequency over time
  Given CCP monitoring records with deviations
  When user views CCP Deviation Trend
  Then bar chart displays:
    - X-axis: Week (or month, based on range)
    - Y-axis: Deviation count
    - Bars stacked by CCP type (Cooking Temp, Cooling Time, Metal Detection, etc.)
    - Tooltip shows CCP name + deviation count
  And table below shows:
    | CCP Name | Deviation Count | Avg Duration | Max Severity |

Scenario: CCP deviation drill-down
  Given user clicks on CCP bar
  Then navigates to /quality/ccp/monitoring?ccp_id={id}&status=deviation
  And shows all deviation records for that CCP
```

### AC-5: SPC Charts (X-bar and R-chart)

```gherkin
Scenario: Display X-bar chart (mean of means)
  Given test result data for parameter "Weight (g)" with subgroups
  When user navigates to Quality > Analytics > SPC Charts
  And selects parameter "Weight (g)"
  Then X-bar chart displays:
    - X-axis: Subgroup number (batch number)
    - Y-axis: Subgroup mean
    - Primary line: Mean of each subgroup (blue dots connected)
    - Center line (X-double-bar): Overall mean (green solid)
    - UCL (Upper Control Limit): X-double-bar + A2 * R-bar (red dashed)
    - LCL (Lower Control Limit): X-double-bar - A2 * R-bar (red dashed)
  And out-of-control points (beyond UCL/LCL) highlighted in red
  And control chart rules applied (Western Electric rules)

Scenario: Display R-chart (range chart)
  Given test result data for parameter "Weight (g)" with subgroups
  When user views R-chart
  Then R-chart displays:
    - X-axis: Subgroup number
    - Y-axis: Subgroup range (max - min)
    - Primary line: Range of each subgroup (blue dots connected)
    - Center line (R-bar): Average range (green solid)
    - UCL: D4 * R-bar (red dashed)
    - LCL: D3 * R-bar (red dashed, may be 0 for small subgroups)
  And out-of-control points highlighted

Scenario: SPC chart process capability
  Given SPC chart displayed
  Then capability metrics panel shows:
    | Metric | Formula | Value |
    | Cpk | Min((USL-mean)/(3*sigma), (mean-LSL)/(3*sigma)) | 1.33 |
    | Ppk | Process performance index | 1.25 |
    | Interpretation | Cpk > 1.33 = Capable process | ✓ Capable |
```

### AC-6: Root Cause Frequency Analysis

```gherkin
Scenario: Display top root causes from NCRs
  Given NCR data with root_cause_category
  When user views Root Cause Frequency analysis
  Then bar chart displays:
    - X-axis: Root cause category (top 10)
    - Y-axis: NCR count
    - Bars sorted by frequency descending
  And data table shows:
    | Root Cause | NCR Count | % of Total | Avg Time to Close |
  And cumulative % line overlaid (Pareto style)

Scenario: Root cause drill-down
  Given user clicks on root cause bar
  Then navigates to /quality/ncr?root_cause={category}
  And shows all NCRs with that root cause
```

### AC-7: Supplier Quality Trends

```gherkin
Scenario: Display supplier quality scorecard trends
  Given supplier quality ratings over time
  When user selects supplier from dropdown
  Then multi-line chart displays:
    - X-axis: Month
    - Y-axis: Score (0-100)
    - Line 1: Defect Rate Score (blue)
    - Line 2: On-Time Delivery Score (green)
    - Line 3: CoA Compliance Score (orange)
    - Line 4: Overall Score (purple, bold)
  And target line at 90% (gray dashed)

Scenario: Compare suppliers side-by-side
  Given user selects multiple suppliers
  When viewing supplier trends
  Then chart shows:
    - Each supplier as a separate line (color-coded)
    - Legend with supplier names
    - Tooltip on hover shows supplier + score
```

### AC-8: CAPA Effectiveness Metrics

```gherkin
Scenario: Display CAPA effectiveness breakdown
  Given CAPA records with effectiveness check results
  When user views CAPA Effectiveness
  Then pie chart displays:
    | Status | Count | % |
    | Effective | 45 | 75% |
    | Ineffective | 10 | 17% |
    | Pending | 5 | 8% |
  And color-coded: Effective=green, Ineffective=red, Pending=yellow

Scenario: CAPA time to close metrics
  Given CAPA records with created_at and closed_at
  Then metrics panel shows:
    | Metric | Value |
    | Avg Time to Close | 14 days |
    | Median Time to Close | 12 days |
    | Overdue CAPAs | 5 (10%) |
    | Closure Rate (last 30 days) | 90% |
  And histogram shows distribution of time to close
```

### AC-9: Analytics Export

```gherkin
Scenario: Export analytics as Excel (multi-sheet)
  Given user viewing analytics dashboard
  When clicking [Export Excel]
  Then Excel file downloads with sheets:
    | Sheet Name | Content |
    | Defect Pareto | Defect type, count, %, cumulative % |
    | NCR Trend | Date, NCR count by severity |
    | Test Trending | Parameter, date, value, UCL, LCL |
    | Root Causes | Root cause, count, % |
    | Supplier Trends | Supplier, month, scores |
    | CAPA Effectiveness | CAPA ID, effectiveness, time to close |
  And filename = "Quality-Analytics-{YYYY-MM-DD}.xlsx"
  And export completes within 15 seconds

Scenario: Export single chart as CSV
  Given user viewing Defect Pareto chart
  When clicking [Export CSV]
  Then CSV downloads with chart data
  And filename = "Defect-Pareto-{YYYY-MM-DD}.csv"
```

### AC-10: Scheduled Reports

```gherkin
Scenario: Schedule daily quality report
  Given user with QA_MANAGER role
  When navigating to Quality > Analytics > Scheduled Reports
  And clicking [Create Schedule]
  Then form opens with fields:
    | Field | Options |
    | Report Type | NCR Trend, Defect Pareto, Supplier Trends, All |
    | Frequency | Daily, Weekly (Mon-Sun), Monthly (1st-31st) |
    | Recipients | Email addresses (comma-separated) |
    | Format | PDF, Excel |
    | Active | Checkbox (default true) |
  And user fills form and clicks [Save]
  Then scheduled report created
  And confirmation: "Report scheduled successfully. First delivery: {date}"

Scenario: Receive scheduled report email
  Given scheduled report configured for daily delivery
  When scheduled time reached (e.g., 8am daily)
  Then email sent to recipients with:
    - Subject: "MonoPilot Quality Report - {date}"
    - Body: Summary of key metrics (text)
    - Attachment: PDF or Excel with full analytics
  And email delivery logged in audit trail
```

### AC-11: Date Range Filtering

```gherkin
Scenario: Filter analytics by date range
  Given user on analytics page
  When selecting date filter:
    | Option | Range |
    | Last 7 Days | Today - 7 days |
    | Last 30 Days | Today - 30 days (default) |
    | Last 90 Days | Today - 90 days |
    | Last 6 Months | Today - 6 months |
    | Last 12 Months | Today - 12 months |
    | YTD | Jan 1 - Today |
    | Custom | User-selected dates |
  Then all charts refresh with filtered data
  And URL updates with date params
```

### AC-12: Mobile Responsiveness

```gherkin
Scenario: Analytics on mobile
  Given user accessing analytics on mobile (< 768px width)
  When page loads
  Then layout adjusts:
    - Charts resize to full width
    - Data tables become scrollable
    - Multi-line charts switch to tabbed view (one line per tab)
    - SPC charts switch to portrait orientation
  And all interactions remain functional
```

### AC-13: Performance Requirements

```gherkin
Scenario: Analytics page load time
  Given organization with 50,000 test results, 5,000 NCRs
  When user loads analytics page
  Then page loads within 2 seconds
  And charts render progressively (skeleton → data)

Scenario: Chart refresh performance
  Given user changes date filter
  When filter applied
  Then all charts refresh within 1 second
  And loading spinners shown during fetch
```

---

## Implementation Notes

### API Endpoints

```typescript
// =============================================================================
// Quality Analytics Endpoints
// =============================================================================

// GET /api/quality/analytics/defect-pareto
interface DefectParetoRequest {
  date_from?: string;
  date_to?: string;
  limit?: number; // Default 10
}

interface DefectParetoResponse {
  data: Array<{
    defect_type: string;
    severity: 'critical' | 'major' | 'minor';
    count: number;
    percentage: number;
    cumulative_percentage: number;
  }>;
}

// GET /api/quality/analytics/ncr-trend
interface NCRTrendRequest {
  date_from?: string;
  date_to?: string;
  bucket?: 'day' | 'week' | 'month';
  by_severity?: boolean; // Default false
}

interface NCRTrendResponse {
  data: Array<{
    date: string;
    total: number;
    critical?: number;
    major?: number;
    minor?: number;
    moving_avg_7d?: number;
  }>;
  trend_direction: 'up' | 'down' | 'stable';
  change_percentage: number;
}

// GET /api/quality/analytics/test-result-trend
interface TestResultTrendRequest {
  parameter_id: string;
  date_from?: string;
  date_to?: string;
}

interface TestResultTrendResponse {
  data: Array<{
    date: string;
    batch_number: string;
    value: number;
    is_out_of_spec: boolean;
  }>;
  statistics: {
    mean: number;
    std_dev: number;
    min: number;
    max: number;
    out_of_spec_count: number;
    out_of_spec_percentage: number;
  };
  control_limits: {
    ucl: number; // Upper Control Limit
    lcl: number; // Lower Control Limit
    target: number;
    spec_min: number;
    spec_max: number;
  };
}

// GET /api/quality/analytics/spc-chart
interface SPCChartRequest {
  parameter_id: string;
  chart_type: 'xbar' | 'r';
  subgroup_size?: number; // Default 5
  date_from?: string;
  date_to?: string;
}

interface SPCChartResponse {
  data: Array<{
    subgroup_number: number;
    batch_number: string;
    mean?: number; // For X-bar
    range?: number; // For R-chart
    is_out_of_control: boolean;
  }>;
  control_limits: {
    center_line: number; // X-double-bar or R-bar
    ucl: number;
    lcl: number;
  };
  capability: {
    cpk: number;
    ppk: number;
    interpretation: string;
  };
}

// GET /api/quality/analytics/root-cause-frequency
interface RootCauseFrequencyResponse {
  data: Array<{
    root_cause_category: string;
    ncr_count: number;
    percentage: number;
    avg_time_to_close_days: number;
  }>;
}

// GET /api/quality/analytics/supplier-quality-trend
interface SupplierQualityTrendRequest {
  supplier_id?: string; // If null, return top 5 suppliers
  date_from?: string;
  date_to?: string;
}

interface SupplierQualityTrendResponse {
  suppliers: Array<{
    supplier_id: string;
    supplier_name: string;
    trend_data: Array<{
      month: string;
      defect_rate_score: number;
      on_time_score: number;
      coa_compliance_score: number;
      overall_score: number;
    }>;
  }>;
}

// GET /api/quality/analytics/capa-effectiveness
interface CAPAEffectivenessResponse {
  breakdown: {
    effective: number;
    ineffective: number;
    pending: number;
  };
  metrics: {
    avg_time_to_close_days: number;
    median_time_to_close_days: number;
    overdue_count: number;
    closure_rate_30d: number;
  };
  time_to_close_histogram: Array<{
    bucket: string; // "0-7 days", "8-14 days", etc.
    count: number;
  }>;
}

// POST /api/quality/analytics/export
interface AnalyticsExportRequest {
  format: 'csv' | 'excel' | 'pdf';
  reports: Array<'defect_pareto' | 'ncr_trend' | 'test_trending' | 'root_cause' | 'supplier_trends' | 'capa_effectiveness'>;
  date_from?: string;
  date_to?: string;
}

interface AnalyticsExportResponse {
  file_url: string;
  filename: string;
  generated_at: string;
}

// POST /api/quality/analytics/schedule-report
interface ScheduleReportRequest {
  report_type: 'ncr_trend' | 'defect_pareto' | 'supplier_trends' | 'all';
  frequency: 'daily' | 'weekly' | 'monthly';
  day_of_week?: number; // 0-6 for weekly
  day_of_month?: number; // 1-31 for monthly
  time?: string; // HH:mm
  recipients: string[]; // Email addresses
  format: 'pdf' | 'excel';
  active: boolean;
}

interface ScheduleReportResponse {
  schedule_id: string;
  next_delivery_at: string;
}
```

### Service Layer

```typescript
// lib/services/quality-analytics-service.ts

export class QualityAnalyticsService {
  // Defect Pareto
  static async getDefectPareto(dateFrom: Date, dateTo: Date, limit: number): Promise<DefectParetoData[]>;

  // NCR Trend
  static async getNCRTrend(
    dateFrom: Date,
    dateTo: Date,
    bucket: 'day' | 'week' | 'month',
    bySeverity: boolean
  ): Promise<NCRTrendData>;

  // Test Result Trend
  static async getTestResultTrend(
    parameterId: string,
    dateFrom: Date,
    dateTo: Date
  ): Promise<TestResultTrendData>;

  // SPC Charts
  static async getSPCChart(
    parameterId: string,
    chartType: 'xbar' | 'r',
    subgroupSize: number,
    dateFrom: Date,
    dateTo: Date
  ): Promise<SPCChartData>;

  static calculateControlLimits(
    data: number[],
    chartType: 'xbar' | 'r',
    subgroupSize: number
  ): { ucl: number; lcl: number; centerLine: number };

  static calculateCpk(
    mean: number,
    stdDev: number,
    specMin: number,
    specMax: number
  ): number;

  // Root Cause
  static async getRootCauseFrequency(dateFrom: Date, dateTo: Date): Promise<RootCauseData[]>;

  // Supplier Trends
  static async getSupplierQualityTrend(
    supplierId: string | null,
    dateFrom: Date,
    dateTo: Date
  ): Promise<SupplierTrendData>;

  // CAPA Effectiveness
  static async getCAPAEffectiveness(dateFrom: Date, dateTo: Date): Promise<CAPAEffectivenessData>;

  // Export
  static async exportAnalytics(
    format: 'csv' | 'excel' | 'pdf',
    reports: string[],
    dateFrom: Date,
    dateTo: Date
  ): Promise<{ url: string; filename: string }>;

  // Scheduled Reports
  static async scheduleReport(schedule: ScheduleReportInput): Promise<string>;
  static async sendScheduledReport(scheduleId: string): Promise<void>;
}
```

### Frontend Components

```
apps/frontend/app/(authenticated)/quality/
  analytics/
    page.tsx                          -- Analytics dashboard page
    defect-pareto/
      page.tsx                        -- Defect Pareto analysis
    ncr-trend/
      page.tsx                        -- NCR trend analysis
    test-trending/
      page.tsx                        -- Test result trending
    spc-charts/
      page.tsx                        -- SPC charts (X-bar, R-chart)
    root-cause/
      page.tsx                        -- Root cause frequency
    supplier-trends/
      page.tsx                        -- Supplier quality trends
    capa-effectiveness/
      page.tsx                        -- CAPA effectiveness

components/quality/analytics/
  QualityAnalyticsDashboard.tsx       -- Analytics dashboard container
  DefectParetoChart.tsx               -- Pareto bar+line chart
  NCRTrendChart.tsx                   -- NCR trend line chart with moving avg
  TestResultTrendChart.tsx            -- Test result trend with control limits
  SPCXBarChart.tsx                    -- X-bar SPC chart
  SPCRChart.tsx                       -- R-chart (range)
  RootCauseFrequencyChart.tsx         -- Root cause bar chart
  SupplierQualityTrendChart.tsx       -- Multi-line supplier trends
  CAPAEffectivenessPieChart.tsx       -- CAPA effectiveness pie chart
  CAPATimeToCloseHistogram.tsx        -- Time to close histogram
  AnalyticsDateFilter.tsx             -- Date range filter
  AnalyticsExportButtons.tsx          -- CSV/Excel/PDF export
  ScheduledReportModal.tsx            -- Scheduled report configuration
  AnalyticsStatisticsPanel.tsx        -- Statistics display panel
```

---

## Key Business Rules

1. **SPC Subgroup Size**: Default subgroup size = 5 (configurable, typically 3-7)
2. **Control Limits Calculation**: X-bar chart uses A2 constant, R-chart uses D3/D4 constants based on subgroup size
3. **Cpk Target**: Cpk > 1.33 considered capable process, Cpk > 1.67 considered highly capable
4. **Moving Average**: 7-day moving average for NCR trend (smooths daily volatility)
5. **Pareto 80/20 Rule**: Highlight top defects contributing to 80% of issues
6. **Scheduled Report Time**: Default 8:00 AM organization timezone
7. **Export File Retention**: Temporary files expire after 24 hours
8. **Date Range Default**: Last 30 days for most analytics (last 90 days for trends)
9. **Minimum Data Points**: Require minimum 30 data points for SPC charts (statistical validity)
10. **Trend Direction Threshold**: >5% change considered "up" or "down", <=5% considered "stable"

---

## Deliverables

### Database
- [ ] Materialized views: defect_pareto_mv, root_cause_frequency_mv (optional, for performance)
- [ ] Refresh function: `refresh_quality_analytics_mvs()`
- [ ] RLS policies for materialized views

### API Routes
- [ ] `GET /api/quality/analytics/defect-pareto`
- [ ] `GET /api/quality/analytics/ncr-trend`
- [ ] `GET /api/quality/analytics/test-result-trend`
- [ ] `GET /api/quality/analytics/spc-chart`
- [ ] `GET /api/quality/analytics/root-cause-frequency`
- [ ] `GET /api/quality/analytics/supplier-quality-trend`
- [ ] `GET /api/quality/analytics/capa-effectiveness`
- [ ] `POST /api/quality/analytics/export`
- [ ] `POST /api/quality/analytics/schedule-report`

### Service Layer
- [ ] All analytics query methods
- [ ] SPC calculations (control limits, Cpk)
- [ ] Export to Excel (multi-sheet)
- [ ] Scheduled report email delivery

### Frontend
- [ ] Analytics dashboard with 7 chart types
- [ ] Date range filter
- [ ] Export buttons (CSV, Excel, PDF)
- [ ] Scheduled report configuration
- [ ] Statistics panels
- [ ] Loading and empty states
- [ ] Mobile responsive layout

### Tests
- [ ] Unit tests: Analytics service methods (>80% coverage)
- [ ] Unit tests: SPC calculations (control limits, Cpk)
- [ ] Integration tests: All API endpoints
- [ ] E2E: Full analytics workflow
- [ ] Performance tests: Analytics load time <2s

---

## Definition of Done

### Database
- [ ] Materialized views created (if used)
- [ ] Refresh function works
- [ ] RLS policies enforce org isolation

### API
- [ ] All 9 endpoints return correct data
- [ ] Response times < 500ms
- [ ] Excel export completes within 15 seconds
- [ ] Scheduled reports sent successfully

### Service
- [ ] All analytics calculations accurate
- [ ] SPC control limits calculated correctly (verified against manual calculations)
- [ ] Cpk calculation matches Six Sigma formula
- [ ] Export includes all requested reports

### Frontend
- [ ] All 7 analytics charts render correctly
- [ ] Charts responsive to date filter changes
- [ ] Export buttons download files successfully
- [ ] Scheduled report modal works
- [ ] Statistics panels display correct values
- [ ] Mobile layout responsive

### Testing
- [ ] Unit tests: >80% coverage
- [ ] SPC calculation tests verified against known values
- [ ] Integration tests: All endpoints covered
- [ ] E2E tests: Full analytics workflow
- [ ] Performance tests: Load time <2s verified

---

## Risk Assessment

| Risk | Impact | Likelihood | Mitigation |
|------|--------|------------|------------|
| SPC calculation complexity | MEDIUM | MEDIUM | Use proven statistical library, verify against manual calculations |
| Performance with large datasets | HIGH | MEDIUM | Materialized views, server-side aggregation, pagination |
| Excel export timeout | MEDIUM | LOW | Background job for exports >10,000 records |
| Chart rendering performance | MEDIUM | LOW | Recharts with lazy loading, limit data points to 500 per chart |
| Scheduled report email delivery | MEDIUM | LOW | Use reliable email service (SendGrid), retry logic |
| Statistical accuracy | HIGH | LOW | Comprehensive unit tests, peer review of formulas |

---

## Version History

| Version | Date | Changes | Author |
|---------|------|---------|--------|
| 1.0 | 2025-01-15 | Initial story creation for Quality Analytics | OPUS |

---

**Document Status**: Ready for Implementation
**Created**: 2025-01-15
**Lines**: ~1000
**Complexity**: L (Large)
**Phase**: 4 (Analytics & Reporting)
