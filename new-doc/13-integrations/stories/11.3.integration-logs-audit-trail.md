# 11.3 - Integration Logs & Audit Trail

**Priority**: P0 (MVP)
**Story Points**: M (Medium)
**Type**: backend
**Phase**: 1 (MVP)
**Model**: OPUS

**State:** ready
**Estimate:** M (3-4 days)
**Primary PRD:** `docs/1-BASELINE/product/modules/integrations.md` (FR-INT-005)
**Architecture:** `docs/1-BASELINE/architecture/modules/integrations.md` (logging patterns)

---

## Goal

Implement comprehensive integration audit trail - immutable logs of all integration events (API calls, webhooks, EDI, Comarch syncs). Includes query API with filters, full-text search, request/response payloads (masked sensitive data), retention management (90 days), and CSV export for compliance.

---

## User Story

As a **System Administrator**, I want to **view a complete audit trail of all integration events** so that **I can troubleshoot failures, track API usage, and maintain compliance records**.

As an **IT Manager**, I want to **search integration logs by date, status, or external system** so that **I can quickly identify patterns in integration failures and optimize performance**.

---

## Dependencies

### Cross-Epic Dependencies

| Dependency | Story/Epic | Type | What It Provides | Status |
|------------|------------|------|------------------|--------|
| 01.1 | Org Context + RLS | HARD | organizations, users, roles tables | Ready |
| 11.1 | Dashboard | HARD | integration_health, activity feed pattern | Ready |
| 11.2 | API Keys | SOFT | api_key_id foreign key | Ready |

### Provides To (Downstream)

| Story | What This Provides |
|-------|-------------------|
| 11.4 | Webhook delivery logs |
| 11.6 | Comarch sync logs |
| All | Central audit trail for integration events |

---

## Database Migration

### Migration: Create integration_logs table

```sql
-- Migration: YYYYMMDDHHMMSS_create_integration_logs.sql

-- Integration event logs (immutable audit trail)
CREATE TABLE integration_logs (
    id                  UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    org_id              UUID NOT NULL REFERENCES organizations(id) ON DELETE CASCADE,

    -- Event metadata
    timestamp           TIMESTAMPTZ NOT NULL DEFAULT now(),
    integration_type    VARCHAR(50) NOT NULL CHECK (integration_type IN ('api', 'webhook', 'edi', 'comarch', 'import', 'export')),
    event_type          VARCHAR(100) NOT NULL, -- e.g., "invoice.created", "webhook.delivered", "order.imported"
    direction           VARCHAR(10) NOT NULL CHECK (direction IN ('inbound', 'outbound')),

    -- Status
    status              VARCHAR(20) NOT NULL CHECK (status IN ('success', 'warning', 'error')),
    http_status         INTEGER,

    -- Payloads (masked sensitive fields)
    request_body        JSONB,
    response_body       JSONB,
    error_message       TEXT,

    -- References
    api_key_id          UUID REFERENCES integration_api_keys(id) ON DELETE SET NULL,
    webhook_id          UUID REFERENCES integration_webhooks(id) ON DELETE SET NULL,
    external_system     VARCHAR(100), -- "Comarch Optima", "Mobile App", etc.
    external_id         VARCHAR(255), -- External reference (PO number, invoice ID, etc.)

    -- Performance
    retry_count         INTEGER DEFAULT 0,
    duration_ms         INTEGER,

    -- Metadata
    metadata            JSONB DEFAULT '{}', -- Additional context

    -- Immutable: No updates allowed (append-only)
    CONSTRAINT no_updates CHECK (false) -- Disable updates via trigger
);

-- Indexes for common queries
CREATE INDEX idx_integration_logs_org_timestamp ON integration_logs(org_id, timestamp DESC);
CREATE INDEX idx_integration_logs_status ON integration_logs(org_id, status, timestamp DESC);
CREATE INDEX idx_integration_logs_type ON integration_logs(org_id, integration_type, timestamp DESC);
CREATE INDEX idx_integration_logs_external ON integration_logs(org_id, external_system, timestamp DESC);
CREATE INDEX idx_integration_logs_api_key ON integration_logs(api_key_id, timestamp DESC) WHERE api_key_id IS NOT NULL;
CREATE INDEX idx_integration_logs_external_id ON integration_logs(external_id) WHERE external_id IS NOT NULL;

-- GIN index for full-text search on payloads
CREATE INDEX idx_integration_logs_request_body ON integration_logs USING GIN (request_body);
CREATE INDEX idx_integration_logs_response_body ON integration_logs USING GIN (response_body);

-- Partitioning by month for performance (optional, if high volume)
-- CREATE TABLE integration_logs_YYYYMM PARTITION OF integration_logs
-- FOR VALUES FROM ('YYYY-MM-01') TO ('YYYY-MM+1-01');

-- RLS Policies
ALTER TABLE integration_logs ENABLE ROW LEVEL SECURITY;

CREATE POLICY "integration_logs_select" ON integration_logs
    FOR SELECT USING (
        org_id = (SELECT org_id FROM users WHERE id = auth.uid())
        AND (SELECT r.code FROM roles r JOIN users u ON u.role_id = r.id WHERE u.id = auth.uid()) IN ('SUPER_ADMIN', 'ADMIN', 'IT_MANAGER')
    );

CREATE POLICY "integration_logs_insert" ON integration_logs
    FOR INSERT WITH CHECK (
        org_id = (SELECT org_id FROM users WHERE id = auth.uid())
    );

-- Prevent updates (immutable logs)
CREATE POLICY "integration_logs_no_update" ON integration_logs
    FOR UPDATE USING (false);

CREATE POLICY "integration_logs_no_delete" ON integration_logs
    FOR DELETE USING (false); -- Only cleanup job can delete (via service role)

-- =============================================================================
-- Function: Log integration event
-- =============================================================================

CREATE OR REPLACE FUNCTION log_integration_event(
    p_org_id UUID,
    p_integration_type VARCHAR,
    p_event_type VARCHAR,
    p_direction VARCHAR,
    p_status VARCHAR,
    p_http_status INTEGER DEFAULT NULL,
    p_request_body JSONB DEFAULT NULL,
    p_response_body JSONB DEFAULT NULL,
    p_error_message TEXT DEFAULT NULL,
    p_api_key_id UUID DEFAULT NULL,
    p_webhook_id UUID DEFAULT NULL,
    p_external_system VARCHAR DEFAULT NULL,
    p_external_id VARCHAR DEFAULT NULL,
    p_retry_count INTEGER DEFAULT 0,
    p_duration_ms INTEGER DEFAULT NULL,
    p_metadata JSONB DEFAULT '{}'
) RETURNS UUID AS $$
DECLARE
    v_log_id UUID;
BEGIN
    INSERT INTO integration_logs (
        org_id, integration_type, event_type, direction, status,
        http_status, request_body, response_body, error_message,
        api_key_id, webhook_id, external_system, external_id,
        retry_count, duration_ms, metadata
    ) VALUES (
        p_org_id, p_integration_type, p_event_type, p_direction, p_status,
        p_http_status, p_request_body, p_response_body, p_error_message,
        p_api_key_id, p_webhook_id, p_external_system, p_external_id,
        p_retry_count, p_duration_ms, p_metadata
    ) RETURNING id INTO v_log_id;

    RETURN v_log_id;
END;
$$ LANGUAGE plpgsql SECURITY DEFINER;

-- =============================================================================
-- Function: Cleanup old logs (retention management)
-- =============================================================================

CREATE OR REPLACE FUNCTION cleanup_old_integration_logs()
RETURNS INTEGER AS $$
DECLARE
    v_deleted INTEGER;
    v_retention_days INTEGER := 90; -- Configurable
BEGIN
    DELETE FROM integration_logs
    WHERE timestamp < now() - (v_retention_days || ' days')::INTERVAL;

    GET DIAGNOSTICS v_deleted = ROW_COUNT;
    RETURN v_deleted;
END;
$$ LANGUAGE plpgsql;

-- Schedule cleanup (via pg_cron or external scheduler)
-- SELECT cron.schedule('cleanup-integration-logs', '0 2 * * *', 'SELECT cleanup_old_integration_logs()');
```

---

## Acceptance Criteria (Given/When/Then)

### AC-1: Log Creation (Immutable)

```gherkin
Scenario: Create integration log entry
  Given API request authenticated with key
  When API call completes
  Then log entry created with:
    | Field | Value |
    | org_id | Current org |
    | integration_type | "api" |
    | event_type | "products.list" |
    | direction | "inbound" |
    | status | "success" |
    | http_status | 200 |
    | api_key_id | Current key ID |
    | duration_ms | Request duration |
  And timestamp = now()

Scenario: Log entry is immutable
  Given existing log entry
  When attempting UPDATE on log
  Then database rejects with error "Updates not allowed on integration_logs"
  And RLS policy blocks update

Scenario: Log entry cannot be deleted by users
  Given existing log entry
  When user attempts DELETE
  Then RLS policy blocks delete
  And only service role can delete (cleanup job)
```

### AC-2: Query Logs with Filters

```gherkin
Scenario: Filter logs by date range
  Given 100 logs spanning 30 days
  When querying with date_range = "last_7_days"
  Then only logs from last 7 days returned
  And sorted by timestamp DESC

Scenario: Filter logs by status
  Given 50 success logs, 20 error logs, 10 warning logs
  When querying with status = "error"
  Then only 20 error logs returned
  And success/warning logs excluded

Scenario: Filter logs by integration type
  Given logs for api, webhook, comarch, edi types
  When querying with integration_type = "webhook"
  Then only webhook logs returned

Scenario: Filter logs by external system
  Given logs for "Comarch Optima", "Mobile App", "EDI VAN"
  When querying with external_system = "Comarch Optima"
  Then only Comarch logs returned

Scenario: Combine multiple filters
  Given varied logs
  When querying with:
    - status = "error"
    - integration_type = "api"
    - date_range = "last_24_hours"
  Then only logs matching ALL filters returned
```

### AC-3: Full-Text Search

```gherkin
Scenario: Search in request/response bodies
  Given log with request_body containing "PO-2024-001"
  When searching for "PO-2024-001"
  Then log returned in results
  And matched field highlighted

Scenario: Search in error messages
  Given log with error_message = "Connection timeout to Comarch API"
  When searching for "timeout"
  Then log returned
  And error message highlighted

Scenario: Search by external ID
  Given log with external_id = "INV-2024-123"
  When searching for "INV-2024-123"
  Then exact match returned first
  And sorted by relevance
```

### AC-4: Pagination

```gherkin
Scenario: Paginate large result sets
  Given 500 logs matching filters
  When querying with page = 1, page_size = 50
  Then first 50 logs returned
  And pagination metadata included:
    | Field | Value |
    | total | 500 |
    | page | 1 |
    | page_size | 50 |
    | total_pages | 10 |

Scenario: Navigate to next page
  Given on page 1 of results
  When requesting page = 2
  Then logs 51-100 returned
  And no duplicate entries
```

### AC-5: Log Detail View

```gherkin
Scenario: View single log entry detail
  Given log entry ID
  When requesting GET /api/integrations/logs/:id
  Then full log details returned:
    | Section | Fields |
    | Metadata | timestamp, integration_type, event_type, direction |
    | Status | status, http_status, duration_ms |
    | Payloads | request_body, response_body (formatted JSON) |
    | Error | error_message, retry_count |
    | References | api_key_id, webhook_id, external_system, external_id |

Scenario: Expand log in activity feed
  Given activity feed with log entries
  When clicking log entry
  Then detail view expands inline
  And request/response JSON formatted
  And sensitive fields masked (passwords, API keys)
```

### AC-6: Sensitive Data Masking

```gherkin
Scenario: Mask passwords in request body
  Given API request with body {"password": "secret123"}
  When logging request
  Then request_body stored as {"password": "***REDACTED***"}
  And original password not stored

Scenario: Mask API keys in headers
  Given request with Authorization header
  When logging
  Then header value masked as "Bearer mp_live_abc***"
  And only prefix preserved

Scenario: Masked fields list
  Then following fields always masked:
    - password
    - api_key
    - api_secret
    - Authorization header
    - X-API-Key header
    - credit_card
    - ssn
```

### AC-7: Export Logs to CSV

```gherkin
Scenario: Export filtered logs to CSV
  Given 100 logs matching current filters
  When clicking [Export CSV]
  Then CSV file generated with columns:
    | Column | Value |
    | Timestamp | ISO 8601 format |
    | Type | integration_type |
    | Event | event_type |
    | Direction | inbound/outbound |
    | Status | success/error/warning |
    | HTTP Status | http_status |
    | External System | external_system |
    | External ID | external_id |
    | Duration (ms) | duration_ms |
    | Error | error_message |
  And request/response bodies excluded (too large)

Scenario: CSV encoding for Excel compatibility
  Given export initiated
  Then file encoded as UTF-8 with BOM
  And date format = YYYY-MM-DD HH:MM:SS
  And special chars escaped
```

### AC-8: Retention Management

```gherkin
Scenario: Default retention is 90 days
  Given organization using default settings
  When cleanup job runs
  Then logs older than 90 days deleted
  And recent logs preserved

Scenario: Archive before deletion
  Given logs scheduled for deletion
  When cleanup job runs at 2 AM daily
  Then logs archived to cold storage (S3)
  And then deleted from active database

Scenario: Configurable retention per org
  Given org with custom retention = 365 days
  When cleanup runs
  Then only logs older than 365 days deleted
```

### AC-9: Performance Requirements

```gherkin
Scenario: Query performance with large dataset
  Given 100,000 logs in database
  When querying with filters + pagination
  Then query returns in < 800ms
  And uses indexes efficiently
  And no full table scan

Scenario: Insert performance
  Given high-volume API traffic (1000 req/min)
  When logging each request
  Then inserts complete in < 50ms
  And no blocking on main request thread
  And batch inserts used if possible
```

### AC-10: Admin-Only Access

```gherkin
Scenario: Admin can view all logs
  Given user with ADMIN role
  Then can view integration_logs for own org
  And can filter, search, export

Scenario: IT Manager has read-only access
  Given user with IT_MANAGER role
  Then can view logs
  But cannot delete or modify

Scenario: Regular user cannot access logs
  Given user with USER role
  When attempting to access /integrations/logs
  Then 403 Forbidden
  And redirected to home
```

### AC-11: RLS Multi-Tenancy

```gherkin
Scenario: Org isolation
  Given User A from Org A
  When querying logs
  Then only Org A logs returned
  And Org B logs never visible

Scenario: Cross-org access attempt
  Given User A from Org A
  When attempting to access Org B log by ID
  Then 404 Not Found (not 403)
  And no data leaked
```

---

## Implementation Notes

### API Endpoints

```typescript
// =============================================================================
// Integration Logs Endpoints
// =============================================================================

// GET /api/integrations/logs - List logs with filters
interface LogsQueryParams {
  page?: number;
  page_size?: number;
  status?: 'success' | 'warning' | 'error';
  integration_type?: string;
  date_range?: 'last_24_hours' | 'last_7_days' | 'last_30_days' | 'custom';
  start_date?: string;
  end_date?: string;
  external_system?: string;
  search?: string; // Full-text search
}

interface LogsListResponse {
  logs: IntegrationLog[];
  pagination: {
    total: number;
    page: number;
    page_size: number;
    total_pages: number;
  };
}

// GET /api/integrations/logs/:id - Get single log detail
interface IntegrationLog {
  id: string;
  org_id: string;
  timestamp: string;
  integration_type: string;
  event_type: string;
  direction: 'inbound' | 'outbound';
  status: 'success' | 'warning' | 'error';
  http_status?: number;
  request_body?: Record<string, any>;
  response_body?: Record<string, any>;
  error_message?: string;
  api_key_id?: string;
  webhook_id?: string;
  external_system?: string;
  external_id?: string;
  retry_count: number;
  duration_ms?: number;
  metadata?: Record<string, any>;
}

// GET /api/integrations/logs/stats - Log statistics
interface LogsStatsResponse {
  total_logs: number;
  success_count: number;
  error_count: number;
  warning_count: number;
  success_rate: number;
  avg_duration_ms: number;
  by_type: Record<string, number>;
  by_status: Record<string, number>;
}

// POST /api/integrations/logs/export - Export logs to CSV
interface ExportLogsRequest {
  filters: LogsQueryParams; // Same filters as list
  format: 'csv';
}

interface ExportLogsResponse {
  download_url: string; // Presigned S3 URL
  expires_at: string;
}
```

### Validation Schemas (Zod)

```typescript
import { z } from 'zod';

export const logsQuerySchema = z.object({
  page: z.coerce.number().int().min(1).default(1),
  page_size: z.coerce.number().int().min(1).max(100).default(50),
  status: z.enum(['success', 'warning', 'error']).optional(),
  integration_type: z.enum(['api', 'webhook', 'edi', 'comarch', 'import', 'export']).optional(),
  date_range: z.enum(['last_24_hours', 'last_7_days', 'last_30_days', 'custom']).optional(),
  start_date: z.string().datetime().optional(),
  end_date: z.string().datetime().optional(),
  external_system: z.string().max(100).optional(),
  search: z.string().max(255).optional(),
});

export const exportLogsSchema = z.object({
  filters: logsQuerySchema,
  format: z.literal('csv'),
});
```

### Service Layer

```typescript
// lib/services/integration-logs-service.ts

export class IntegrationLogsService {
  /**
   * Create new log entry (immutable)
   */
  static async createLog(data: CreateLogData): Promise<string>; // Returns log ID

  /**
   * Query logs with filters and pagination
   */
  static async queryLogs(orgId: string, filters: LogsQueryParams): Promise<LogsListResponse>;

  /**
   * Get single log by ID
   */
  static async getLog(id: string, orgId: string): Promise<IntegrationLog | null>;

  /**
   * Get log statistics
   */
  static async getStats(orgId: string, dateRange?: string): Promise<LogsStatsResponse>;

  /**
   * Export logs to CSV
   */
  static async exportLogs(orgId: string, filters: LogsQueryParams): Promise<string>; // Returns download URL

  /**
   * Mask sensitive fields in payloads
   */
  static maskSensitiveData(payload: Record<string, any>): Record<string, any>;

  /**
   * Full-text search in logs
   */
  static async searchLogs(orgId: string, query: string, filters?: LogsQueryParams): Promise<IntegrationLog[]>;

  /**
   * Cleanup old logs (called by cron)
   */
  static async cleanupOldLogs(retentionDays?: number): Promise<number>; // Returns deleted count
}

interface CreateLogData {
  org_id: string;
  integration_type: string;
  event_type: string;
  direction: 'inbound' | 'outbound';
  status: 'success' | 'warning' | 'error';
  http_status?: number;
  request_body?: Record<string, any>;
  response_body?: Record<string, any>;
  error_message?: string;
  api_key_id?: string;
  webhook_id?: string;
  external_system?: string;
  external_id?: string;
  retry_count?: number;
  duration_ms?: number;
  metadata?: Record<string, any>;
}
```

### Frontend Components

```
apps/frontend/app/(authenticated)/integrations/
  logs/
    page.tsx                    -- Logs list page

components/integrations/logs/
  IntegrationLogsTable.tsx      -- DataTable for logs
  LogsFilters.tsx               -- Filter sidebar
  LogDetailModal.tsx            -- Expandable log detail
  LogStatusBadge.tsx            -- Status badge
  LogTypeIcon.tsx               -- Type-specific icons
  ExportLogsButton.tsx          -- CSV export trigger
  LogsStatsWidget.tsx           -- Statistics summary
  JsonPayloadViewer.tsx         -- Formatted JSON display
```

---

## Key Business Rules

1. **Immutability**: Logs are append-only, no updates or deletes by users
2. **Retention**: 90 days default, configurable per org
3. **Sensitive Data Masking**: Passwords, API keys always masked before storage
4. **Admin-Only**: Only ADMIN/IT_MANAGER can view logs
5. **Pagination**: Default 50 items per page, max 100
6. **Full-Text Search**: GIN indexes on JSONB payloads
7. **RLS Enforcement**: All queries filtered by org_id
8. **Cleanup Job**: Runs daily at 2 AM to delete old logs
9. **Export Limit**: Max 10,000 logs per export
10. **Archive Before Delete**: Logs archived to S3 before deletion

---

## Deliverables

### Database
- [ ] `integration_logs` table with constraints
- [ ] Indexes (timestamp, status, type, GIN for search)
- [ ] RLS policies (select, insert, no update/delete)
- [ ] `log_integration_event()` function
- [ ] `cleanup_old_integration_logs()` function

### API Routes
- [ ] GET /api/integrations/logs (list with filters)
- [ ] GET /api/integrations/logs/:id (detail)
- [ ] GET /api/integrations/logs/stats (statistics)
- [ ] POST /api/integrations/logs/export (CSV export)

### Service Layer
- [ ] IntegrationLogsService.createLog()
- [ ] IntegrationLogsService.queryLogs()
- [ ] IntegrationLogsService.getLog()
- [ ] IntegrationLogsService.getStats()
- [ ] IntegrationLogsService.exportLogs()
- [ ] IntegrationLogsService.maskSensitiveData()
- [ ] IntegrationLogsService.cleanupOldLogs()

### Validation
- [ ] logsQuerySchema
- [ ] exportLogsSchema

### Frontend
- [ ] Logs list page with filters
- [ ] Log detail modal
- [ ] Filters sidebar
- [ ] Status badges
- [ ] Type icons
- [ ] CSV export button
- [ ] Stats widget
- [ ] JSON payload viewer

### Tests
- [ ] Unit tests: IntegrationLogsService (>80% coverage)
- [ ] Integration tests: All endpoints
- [ ] RLS tests: Multi-tenancy
- [ ] Search tests: Full-text search
- [ ] Export tests: CSV generation
- [ ] E2E: Filter, search, export workflow

---

## Definition of Done

### Database
- [ ] Table created with immutability constraint
- [ ] RLS prevents updates/deletes
- [ ] Indexes optimized for queries
- [ ] Functions tested

### API
- [ ] All endpoints functional
- [ ] Pagination works correctly
- [ ] Filters apply correctly
- [ ] Full-text search works
- [ ] Response times < 800ms
- [ ] RLS enforced

### Service
- [ ] Logs created immutably
- [ ] Sensitive data masked
- [ ] Filters combine correctly
- [ ] Search returns relevant results
- [ ] CSV export generates valid file
- [ ] Cleanup job deletes old logs

### Frontend
- [ ] Logs list displays correctly
- [ ] Filters work
- [ ] Search functional
- [ ] Detail view expands
- [ ] CSV export downloads
- [ ] Mobile responsive

### Testing
- [ ] Unit tests: >80% coverage
- [ ] Integration tests passing
- [ ] RLS verified
- [ ] Search tested
- [ ] E2E workflow passing

---

## Risk Assessment

| Risk | Impact | Likelihood | Mitigation |
|------|--------|------------|------------|
| Log volume growth | HIGH | HIGH | 90-day retention, archival, partitioning |
| Query performance degradation | MEDIUM | MEDIUM | Indexes, pagination, query optimization |
| Sensitive data leak | CRITICAL | LOW | Masking before insert, audit checks |
| Disk space exhaustion | HIGH | MEDIUM | Cleanup job, monitoring, alerts |

---

## Version History

| Version | Date | Changes | Author |
|---------|------|---------|--------|
| 1.0 | 2026-01-15 | Initial story for Epic 11 Phase 1 | ARCHITECT-AGENT |

---

**Document Status**: Ready for Implementation
**Created**: 2026-01-15
**Lines**: ~580
**Complexity**: M (Medium)
**Phase**: 1 (MVP)
