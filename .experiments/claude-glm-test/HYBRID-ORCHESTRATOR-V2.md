# HYBRID ORCHESTRATOR V2 - Parallel + GLM Integration

**Combines**: MASTER-PROMPT parallel execution + GLM-4.7 hybrid approach

---

## ğŸ¯ KEY IMPROVEMENTS

### vs V1 (Sequential):
1. âœ… **Parallel Story Execution** - Process 2-3 stories simultaneously
2. âœ… **Agent-Internal GLM** - Agenci uÅ¼ywajÄ… GLM transparentnie (nie musisz rÄ™cznie wywoÅ‚ywaÄ‡)
3. âœ… **Simplified Monitoring** - 1 skrypt zamiast 5
4. âœ… **Same MASTER-PROMPT Flow** - Znany workflow, tylko agenci uÅ¼ywajÄ… GLM pod maskÄ…

### Time Savings:
- **V1 Sequential**: 3 stories Ã— 1.5h = **4.5h**
- **V2 Parallel**: 3 stories in **~2h** (prawie 2x szybciej!)

---

## ğŸ“‹ 7-PHASE FLOW (Parallel)

| Phase | Agent | GLM Usage | Parallel | Skip When |
|-------|-------|-----------|----------|-----------|
| **P1** | ux-designer | No (Claude) | âœ“ Multi-story | Backend-only |
| **P2** | test-writer | **âœ… GLM-4.7 internally** | âœ“ Multi-story | Never |
| **P3** | backend/frontend-dev | **âœ… GLM-4.7 internally** | âœ“ Multi-story + tracks | - |
| **P4** | senior-dev | GLM-4.5-Air | No | Clean code |
| **P5** | code-reviewer | No (Claude CRITICAL) | âœ“ Multi-story | Never |
| **P6** | qa-agent | No (Claude) | âœ“ Multi-story | Never |
| **P7** | tech-writer | **âœ… GLM-4.5-Air internally** | âœ“ Multi-story | Never |

**Key**: Agenci **wewnÄ™trznie** wywoÅ‚ujÄ… GLM, Ty tylko delegujesz `Task(test-writer)`.

---

## ğŸ”§ AGENT CONFIGURATION

### Agenci UÅ¼ywajÄ…cy GLM (Pod MaskÄ…):

```yaml
test-writer:
  model: "haiku"  # Cheap orchestrator
  internal_glm:
    model: "glm-4.7"
    thinking: true
    use_for: "test generation"
  process:
    1. Claude (haiku) tworzy prompt dla GLM
    2. WywoÅ‚uje glm_call_updated.py --model glm-4.7 --thinking
    3. Waliduje output
    4. Zwraca testy do checkpointa

backend-dev / frontend-dev:
  model: "haiku"  # Cheap orchestrator
  internal_glm:
    model: "glm-4.7"
    thinking: true
    use_for: "code generation"
  process:
    1. Claude (haiku) tworzy implementation spec
    2. WywoÅ‚uje GLM-4.7 dla kaÅ¼dego pliku
    3. Waliduje skÅ‚adniÄ™ TypeScript
    4. Zwraca kod do checkpointa

tech-writer:
  model: "haiku"
  internal_glm:
    model: "glm-4.5-air"
    thinking: false
    use_for: "documentation"
  process:
    1. Claude tworzy doc structure
    2. GLM-4.5-Air generuje content
    3. Claude formatuje
```

**KorzyÅ›Ä‡**: **Transparentne dla Ciebie** - wywoÅ‚ujesz `Task(test-writer)` normalnie, agent sam uÅ¼yje GLM.

---

## âš¡ PARALLEL EXECUTION (jak MASTER-PROMPT)

### PrzykÅ‚ad: 3 Stories RÃ³wnolegle

```
Phase P1 (UX Design):
â”œâ”€ Story 01.2 (Claude UX) â”€â”€â”
â”œâ”€ Story 01.3 (Claude UX) â”€â”€â”¤ Parallel (3 agents)
â””â”€ Story 01.4 (Claude UX) â”€â”€â”˜

Phase P2 (Tests):
â”œâ”€ Story 01.2 (test-writer â†’ GLM-4.7) â”€â”€â”
â”œâ”€ Story 01.3 (test-writer â†’ GLM-4.7) â”€â”€â”¤ Parallel
â””â”€ Story 01.4 (test-writer â†’ GLM-4.7) â”€â”€â”˜

Phase P3 (Implementation):
â”œâ”€ Story 01.2 (backend-dev â†’ GLM-4.7) â”€â”€â”
â”œâ”€ Story 01.3 (backend-dev â†’ GLM-4.7) â”€â”€â”¤ Parallel
â””â”€ Story 01.4 (backend-dev â†’ GLM-4.7) â”€â”€â”˜

Phase P5 (Code Review):
â”œâ”€ Story 01.2 (code-reviewer â†’ Claude) â”€â”€â”
â”œâ”€ Story 01.3 (code-reviewer â†’ Claude) â”€â”€â”¤ Parallel
â””â”€ Story 01.4 (code-reviewer â†’ Claude) â”€â”€â”˜

... itd
```

**Czas**: ~2h zamiast 4.5h âš¡

---

## ğŸ“¤ SIMPLIFIED DELEGATION

### Jak WywoÅ‚ywaÄ‡ (Ten Sam SposÃ³b co MASTER-PROMPT):

```
Task(test-writer): Story 01.2 P2
Do: Write comprehensive tests for User Roles CRUD
Read: docs/2-MANAGEMENT/epics/current/01-settings/01.2.user-roles.md
Exit: Tests written, checkpoint updated

Task(backend-dev): Story 01.2 P3
Do: Implement User Roles service + API routes
Read: .claude/checkpoints/01.2.yaml
Exit: Code complete, tests passing

Task(code-reviewer): Story 01.2 P5
Do: Review implementation, check for bugs
Read: .claude/checkpoints/01.2.yaml
Exit: APPROVED or REQUEST_CHANGES
```

**Nie musisz** wywoÅ‚ywaÄ‡ GLM rÄ™cznie - agenci robiÄ… to pod maskÄ…!

---

## ğŸ”„ AGENT IMPLEMENTATION (Simplified)

### test-writer Agent (przykÅ‚ad):

```python
# Agent internally:
def execute_p2_test_writing(story_id):
    # Step 1: Load story context
    story = load_story(story_id)

    # Step 2: Create GLM prompt (Claude haiku orchestrates)
    glm_prompt = f"""
    Write Vitest tests for {story['name']}.

    Requirements:
    - Service tests: {story['service_functions']}
    - API tests: {story['api_endpoints']}
    - 50+ test cases

    Context:
    {load_file('.claude/PATTERNS.md')}
    """

    # Step 3: Call GLM (internal)
    result = subprocess.run([
        'python', 'glm_call_updated.py',
        '-m', 'glm-4.7',
        '--thinking',
        '-p', glm_prompt,
        '-o', f'P2_tests_{story_id}.test.ts'
    ])

    # Step 4: Validate & checkpoint
    update_checkpoint(story_id, 'P2', tokens=result.tokens)

    return f"P2: âœ“ test-writer | tests: {count_tests()} | tokens: {result.tokens}"
```

**Z Twojej perspektywy**: WywoÅ‚ujesz `Task(test-writer)`, agent robi resztÄ™.

---

## ğŸ“Š SIMPLIFIED MONITORING

### Jeden Skrypt Zamiast 5:

```bash
# .experiments/claude-glm-test/scripts/hybrid_monitor.py

# UÅ¼ycie:
python hybrid_monitor.py --story 01.2 --action all

# --action all robi:
#   1. Record metrics
#   2. Check regressions
#   3. Compare to baseline
#   4. Update dashboard
#   5. Quality gate check
#
# Output: âœ… PASS / âŒ FAIL + dashboard.html
```

**Zamiast**:
```bash
python monitor_quality.py --story 01.2 --scenario b
python detect_regressions.py --story 01.2 --scenario b
python compare_before_after.py ...
python quality_dashboard.py ...
./quality_gate.sh ...
```

**1 komenda** vs 5 komend âœ…

---

## ğŸ¯ START PROMPT (V2 - Parallel)

```
Execute Epic 01-Settings using HYBRID approach with PARALLEL execution.

Stories: 01.2 (User Roles), 01.3 (Permissions), 01.4 (Org Profile)

Use PARALLEL workflow (like MASTER-PROMPT):
- Launch P1 for all 3 stories in parallel (3x ux-designer agents)
- Launch P2 for all 3 stories in parallel (3x test-writer agents â†’ internal GLM-4.7)
- Launch P3 for all 3 stories in parallel (3x backend-dev â†’ internal GLM-4.7)
- Launch P5 for all 3 stories in parallel (3x code-reviewer â†’ Claude)
- Iterate P3â†’P5 until APPROVED
- Launch P6 for all 3 stories in parallel (3x qa-agent â†’ Claude)
- Launch P7 for all 3 stories in parallel (3x tech-writer â†’ GLM-4.5-Air)

Agents use GLM internally (transparent). You orchestrate with Task() delegations.

Track tokens in .claude/checkpoints/{story}.yaml per phase.

After all 3 stories complete:
python .experiments/claude-glm-test/scripts/hybrid_monitor.py --stories 01.2,01.3,01.4 --action report

Cost target: ~$0.60 (3 stories)
Quality target: 10/10 ACs per story
Time target: ~2h (parallel execution)

START. NO QUESTIONS. DELEGATE ALL P1s IN PARALLEL.
```

---

## ğŸ¤” Pytanie do Ciebie

**KtÃ³ra wersja wolisz?**

### **Opcja A: HYBRID-V1 (Sequential, RÄ™czne GLM)**
- âœ… PeÅ‚na kontrola (widzisz kaÅ¼de wywoÅ‚anie GLM)
- âœ… Åatwe do zrozumienia
- âŒ Musisz rÄ™cznie wywoÅ‚ywaÄ‡ `glm_call_updated.py`
- âŒ Sekwencyjne (1 story na raz)
- âŒ Wolniejsze (~4.5h dla 3 stories)

### **Opcja B: HYBRID-V2 (Parallel, GLM w Agentach)** â­ **RECOMMENDED**
- âœ… Parallel execution (2-3 stories na raz)
- âœ… Agenci uÅ¼ywajÄ… GLM transparentnie
- âœ… Znany workflow (Task() delegations jak MASTER-PROMPT)
- âœ… Szybsze (~2h dla 3 stories)
- âŒ Wymaga modyfikacji agentÃ³w (test-writer, backend-dev, tech-writer)

### **Opcja C: Hybrydowa** (Start V1, pÃ³Åºniej V2)
- Pilot 3 stories z V1 (manual, sequential) â†’ **Validate savings**
- NastÄ™pnie refactor agentÃ³w dla V2 â†’ **Scale with parallel**

---

JeÅ›li chcesz **OpcjÄ™ B** (parallel + agenci z GLM), mogÄ™:
1. âœ… StworzyÄ‡ **consolidated monitoring script** (`hybrid_monitor.py` - 1 zamiast 5)
2. âœ… ZaktualizowaÄ‡ **agentÃ³w** (test-writer, backend-dev, tech-writer) Å¼eby uÅ¼ywali GLM wewnÄ™trznie
3. âœ… StworzyÄ‡ **HYBRID-ORCHESTRATOR-V2-FINAL.md** z parallel workflow

**KtÃ³ra opcja?** A / B / C ?