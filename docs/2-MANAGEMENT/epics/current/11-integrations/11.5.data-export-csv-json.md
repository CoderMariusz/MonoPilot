# 11.5 - Data Export (CSV/JSON)

**Priority**: P0 (MVP)
**Story Points**: M (Medium)
**Type**: backend
**Phase**: 1 (MVP)
**Model**: OPUS

**State:** ready
**Estimate:** M (2-3 days)
**Primary PRD:** `docs/1-BASELINE/product/modules/integrations.md` (FR-INT-008)
**Architecture:** `docs/1-BASELINE/architecture/modules/integrations.md` (export patterns)

---

## Goal

Implement bulk data export functionality - allow users to export products, BOMs, purchase orders, work orders, inventory, and shipments to CSV or JSON formats. Includes async job queue for large exports (>10k rows), S3 presigned URLs for downloads (24h expiry), UTF-8 encoding with BOM for Excel compatibility, and email notifications when ready.

---

## User Story

As a **System Administrator**, I want to **export MonoPilot data to CSV or JSON files** so that **I can import data into external systems, create backups, or perform custom analysis in Excel/BI tools**.

As an **Operations Manager**, I want to **filter export data by date range and status** so that **I only export relevant records without downloading the entire dataset**.

---

## Dependencies

### Cross-Epic Dependencies

| Dependency | Story/Epic | Type | What It Provides | Status |
|------------|------------|------|------------------|--------|
| 01.1 | Org Context + RLS | HARD | organizations, users, roles tables | Ready |
| 02.1 | Products & BOMs | SOFT | Products/BOMs data to export | Ready |
| 03.x | Planning | SOFT | POs, WOs data to export | Partial |
| 11.1 | Dashboard | HARD | Nav shell, integration health | Ready |
| 11.3 | Integration Logs | HARD | Export logging | Ready |

### Provides To (Downstream)

| Story | What This Provides |
|-------|-------------------|
| All | Bulk data export capability |
| Integrations | Data migration support |

---

## Database Migration

### Migration: Create export_jobs table

```sql
-- Migration: YYYYMMDDHHMMSS_create_export_jobs.sql

-- Export job tracking (for async exports)
CREATE TABLE export_jobs (
    id                  UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    org_id              UUID NOT NULL REFERENCES organizations(id) ON DELETE CASCADE,

    -- Job configuration
    entity_type         VARCHAR(50) NOT NULL CHECK (entity_type IN ('products', 'boms', 'purchase_orders', 'work_orders', 'inventory', 'shipments', 'suppliers')),
    format              VARCHAR(10) NOT NULL CHECK (format IN ('csv', 'json')),
    filters             JSONB DEFAULT '{}', -- Date range, status, etc.

    -- Job status
    status              VARCHAR(20) NOT NULL DEFAULT 'pending' CHECK (status IN ('pending', 'processing', 'completed', 'failed')),
    progress            INTEGER DEFAULT 0, -- Percentage (0-100)
    total_rows          INTEGER,
    processed_rows      INTEGER DEFAULT 0,

    -- Output
    file_url            TEXT, -- S3 presigned URL
    file_size_bytes     BIGINT,
    expires_at          TIMESTAMPTZ, -- URL expiry (24 hours)

    -- Error handling
    error_message       TEXT,

    -- Audit
    created_by          UUID REFERENCES users(id),
    created_at          TIMESTAMPTZ NOT NULL DEFAULT now(),
    started_at          TIMESTAMPTZ,
    completed_at        TIMESTAMPTZ,
    updated_at          TIMESTAMPTZ NOT NULL DEFAULT now()
);

-- Indexes
CREATE INDEX idx_export_jobs_org ON export_jobs(org_id, created_at DESC);
CREATE INDEX idx_export_jobs_status ON export_jobs(status, created_at) WHERE status IN ('pending', 'processing');
CREATE INDEX idx_export_jobs_user ON export_jobs(created_by, created_at DESC);

-- RLS Policies
ALTER TABLE export_jobs ENABLE ROW LEVEL SECURITY;

CREATE POLICY "export_jobs_select" ON export_jobs
    FOR SELECT USING (
        org_id = (SELECT org_id FROM users WHERE id = auth.uid())
    );

CREATE POLICY "export_jobs_insert" ON export_jobs
    FOR INSERT WITH CHECK (
        org_id = (SELECT org_id FROM users WHERE id = auth.uid())
    );

CREATE POLICY "export_jobs_update" ON export_jobs
    FOR UPDATE USING (
        org_id = (SELECT org_id FROM users WHERE id = auth.uid())
    );

-- Trigger for updated_at
CREATE TRIGGER update_export_jobs_updated_at
    BEFORE UPDATE ON export_jobs
    FOR EACH ROW
    EXECUTE FUNCTION update_updated_at_column();

-- =============================================================================
-- Function: Cleanup expired export files
-- =============================================================================

CREATE OR REPLACE FUNCTION cleanup_expired_exports()
RETURNS INTEGER AS $$
DECLARE
    v_deleted INTEGER;
BEGIN
    -- Mark expired exports as failed
    UPDATE export_jobs
    SET status = 'failed',
        error_message = 'Export expired (24 hours)'
    WHERE status = 'completed'
      AND expires_at < now();

    GET DIAGNOSTICS v_deleted = ROW_COUNT;
    RETURN v_deleted;
END;
$$ LANGUAGE plpgsql;

-- Schedule cleanup (via pg_cron or external scheduler)
-- SELECT cron.schedule('cleanup-exports', '0 */6 * * *', 'SELECT cleanup_expired_exports()');
```

---

## Acceptance Criteria (Given/When/Then)

### AC-1: Export Entity Selection

```gherkin
Scenario: Select entity to export
  Given user navigates to /integrations/export
  Then entity selector shows:
    | Entity | Description |
    | Products | All products with BOMs |
    | BOMs | Bill of materials (all versions) |
    | Purchase Orders | POs with line items |
    | Work Orders | Production orders with consumption |
    | Inventory | Current stock levels by location |
    | Shipments | Delivery notes and tracking |
    | Suppliers | Supplier contact details |
  And checkboxes for selection

Scenario: Select format
  Then format dropdown shows:
    - CSV (Excel compatible)
    - JSON (API compatible)
  And default = "CSV"
```

### AC-2: Export Filters

```gherkin
Scenario: Filter by date range
  Given exporting "Purchase Orders"
  When selecting date_range = "last_30_days"
  Then only POs created in last 30 days exported
  And filter stored in filters JSONB

Scenario: Custom date range
  Given exporting with custom date range
  When setting:
    - start_date = "2024-01-01"
    - end_date = "2024-12-31"
  Then only records between dates exported

Scenario: Filter by status
  Given exporting "Work Orders"
  When selecting status = ["completed", "closed"]
  Then only completed/closed WOs exported
  And draft/pending WOs excluded

Scenario: Include/exclude archived
  Given export options
  When include_archived = false (default)
  Then archived records excluded
  When include_archived = true
  Then archived records included
```

### AC-3: CSV Export Format

```gherkin
Scenario: CSV export - products
  Given exporting products to CSV
  Then columns include:
    | Column | Example |
    | product_code | "P-001" |
    | name | "Bread Loaf White 500g" |
    | type | "finished_good" |
    | unit | "pcs" |
    | description | "White bread loaf..." |
    | shelf_life_days | 7 |
    | created_at | "2024-12-15" |
  And headers in first row
  And UTF-8 encoding with BOM (Excel compatible)
  And date format = YYYY-MM-DD
  And decimal separator = . (period)

Scenario: CSV export - nested data flattened
  Given product has allergens array
  Then allergens exported as comma-separated string
  Example: "gluten,eggs,milk"
  And nested objects flattened:
    - supplier.name → supplier_name
    - location.warehouse → location_warehouse
```

### AC-4: JSON Export Format

```gherkin
Scenario: JSON export - products
  Given exporting products to JSON
  Then output structure:
    {
      "export_metadata": {
        "entity_type": "products",
        "exported_at": "2025-01-15T10:30:00Z",
        "total_records": 150,
        "filters": {...}
      },
      "data": [
        {
          "id": "uuid",
          "product_code": "P-001",
          "name": "Bread Loaf White 500g",
          "type": "finished_good",
          "allergens": ["gluten", "eggs"],
          "created_at": "2024-12-15T08:00:00Z"
        }
      ]
    }
  And pretty-printed (indented)
  And ISO 8601 timestamps
  And nested objects preserved
```

### AC-5: Async Export for Large Datasets

```gherkin
Scenario: Small export (< 1000 rows) - synchronous
  Given exporting 500 products
  When clicking [Export]
  Then file generated immediately
  And download starts automatically
  And no job record created

Scenario: Large export (> 1000 rows) - async job
  Given exporting 15,000 products
  When clicking [Export]
  Then export_jobs record created
  And status = "pending"
  And background job queued
  And user redirected to /integrations/export/jobs/:id
  And progress indicator shown

Scenario: Job processing updates progress
  Given export job processing
  When background worker processes rows
  Then progress updated every 1000 rows:
    - processed_rows = 5000
    - progress = 33% (5000/15000)
  And UI polls every 2s for status update
  And progress bar updates in real-time

Scenario: Job completion
  Given export job processing
  When all rows processed
  Then status = "completed"
  And file uploaded to S3
  And file_url = presigned S3 URL (valid 24h)
  And email sent to created_by user
  And completed_at = now()
```

### AC-6: Download File

```gherkin
Scenario: Download completed export
  Given export job status = "completed"
  When clicking [Download] button
  Then browser downloads file from file_url (S3)
  And filename = "{entity}_{date}_{format}.{ext}"
  Example: "products_2025-01-15_csv.csv"

Scenario: Download link expires after 24h
  Given export completed at "2025-01-15 10:00:00"
  When attempting download at "2025-01-16 10:00:01"
  Then 403 Forbidden (S3 URL expired)
  And message "Export link expired - please re-export"

Scenario: Re-export after expiry
  Given expired export job
  When clicking [Re-export]
  Then new export job created
  And uses same filters as original
  And new file generated
```

### AC-7: Email Notification

```gherkin
Scenario: Email sent on completion
  Given large export job completed
  When file uploaded to S3
  Then email sent to user who created export
  And email contains:
    - Entity type and row count
    - Download link (presigned URL)
    - Expiry warning (24 hours)
    - Filters applied
  And subject = "Export ready: {entity_type} ({total_rows} records)"

Scenario: Email on failure
  Given export job failed
  Then email sent with:
    - Error message
    - Suggestion to retry or contact support
  And subject = "Export failed: {entity_type}"
```

### AC-8: Export Jobs List

```gherkin
Scenario: View export jobs history
  Given user navigates to /integrations/export/jobs
  Then table displays user's export jobs:
    | Column | Content |
    | Created | "2025-01-15 10:30 AM" |
    | Entity | "Products" |
    | Format | "CSV" |
    | Rows | "15,000" |
    | Status | Badge (pending/processing/completed/failed) |
    | Progress | "67%" (if processing) |
    | Actions | [Download] [Re-export] |
  And sorted by created_at DESC

Scenario: Filter jobs by status
  Given jobs list
  When filtering by status = "completed"
  Then only completed jobs shown
```

### AC-9: Error Handling

```gherkin
Scenario: Export job fails
  Given export job processing
  When database query timeout
  Then status = "failed"
  And error_message = "Query timeout - try smaller date range"
  And user notified via email
  And can retry export

Scenario: S3 upload failure
  Given export file generated
  When S3 upload fails
  Then status = "failed"
  And error_message = "Failed to upload file - please retry"
  And file deleted locally
  And retry available
```

### AC-10: Export Limits & Validation

```gherkin
Scenario: Maximum export size
  Given exporting with filters
  When estimated row count > 100,000
  Then warning shown "Export too large - please narrow filters"
  And export blocked until filters adjusted

Scenario: No data to export
  Given filters result in 0 records
  When clicking [Export]
  Then error "No data matches filters - adjust filters"
  And export not created

Scenario: Required filters
  Given exporting "Purchase Orders"
  When date_range not selected
  Then validation error "Date range is required"
  And form submission prevented
```

### AC-11: Permission Enforcement

```gherkin
Scenario: User can export own org data only
  Given user from Org A
  When exporting
  Then only Org A data exported
  And RLS filters by org_id

Scenario: Admin can export all entities
  Given user with ADMIN role
  Then can export any entity type
  And all export features available

Scenario: User role has limited export
  Given user with USER role
  Then can export products, BOMs only
  But cannot export POs, WOs, inventory
  And restricted entities hidden
```

### AC-12: Export Configuration Per Entity

```gherkin
Scenario: Products export columns
  Then includes:
    - product_code, name, type, unit, description
    - allergens, shelf_life_days, storage_temp
    - created_at, updated_at
    - supplier_id, supplier_name (if applicable)

Scenario: Purchase Orders export columns
  Then includes:
    - po_number, po_date, supplier_name
    - delivery_date, status, total_amount
    - Line items: product_code, qty, unit_price
    - Flattened line items (1 row per line)

Scenario: Inventory export columns
  Then includes:
    - product_code, product_name
    - location_warehouse, location_zone
    - license_plate_number
    - qty_available, unit
    - batch_number, expiry_date
    - last_movement_date
```

---

## Implementation Notes

### API Endpoints

```typescript
// =============================================================================
// Data Export Endpoints
// =============================================================================

// POST /api/integrations/export - Create export job
interface CreateExportRequest {
  entity_type: 'products' | 'boms' | 'purchase_orders' | 'work_orders' | 'inventory' | 'shipments' | 'suppliers';
  format: 'csv' | 'json';
  filters?: {
    date_range?: string;
    start_date?: string;
    end_date?: string;
    status?: string[];
    include_archived?: boolean;
  };
}

interface CreateExportResponse {
  job_id: string;
  status: 'pending' | 'completed'; // Immediate or async
  download_url?: string; // If immediate
  estimated_rows?: number;
}

// GET /api/integrations/export/jobs - List export jobs
interface ExportJobsListResponse {
  jobs: ExportJob[];
  pagination: PaginationMeta;
}

// GET /api/integrations/export/jobs/:id - Get job status
interface ExportJob {
  id: string;
  entity_type: string;
  format: string;
  status: 'pending' | 'processing' | 'completed' | 'failed';
  progress: number; // 0-100
  total_rows?: number;
  processed_rows?: number;
  file_url?: string;
  file_size_bytes?: number;
  expires_at?: string;
  error_message?: string;
  created_at: string;
  completed_at?: string;
}

// POST /api/integrations/export/jobs/:id/retry - Retry failed job
```

### Validation Schemas (Zod)

```typescript
import { z } from 'zod';

export const entityTypeEnum = z.enum(['products', 'boms', 'purchase_orders', 'work_orders', 'inventory', 'shipments', 'suppliers']);
export const exportFormatEnum = z.enum(['csv', 'json']);

export const createExportSchema = z.object({
  entity_type: entityTypeEnum,
  format: exportFormatEnum.default('csv'),
  filters: z.object({
    date_range: z.enum(['last_7_days', 'last_30_days', 'last_90_days', 'last_365_days', 'custom']).optional(),
    start_date: z.string().datetime().optional(),
    end_date: z.string().datetime().optional(),
    status: z.array(z.string()).optional(),
    include_archived: z.boolean().default(false),
  }).optional(),
}).refine(
  (data) => {
    // If date_range = 'custom', start_date and end_date required
    if (data.filters?.date_range === 'custom') {
      return data.filters.start_date && data.filters.end_date;
    }
    return true;
  },
  { message: 'start_date and end_date required for custom date range' }
);
```

### Service Layer

```typescript
// lib/services/export-service.ts

export class ExportService {
  /**
   * Create export job (async for large datasets)
   */
  static async createExport(orgId: string, userId: string, config: CreateExportRequest): Promise<CreateExportResponse>;

  /**
   * Process export job (background worker)
   */
  static async processExportJob(jobId: string): Promise<void>;

  /**
   * Export products to CSV
   */
  static async exportProductsCSV(orgId: string, filters?: Filters): Promise<string>; // Returns CSV string

  /**
   * Export products to JSON
   */
  static async exportProductsJSON(orgId: string, filters?: Filters): Promise<object>;

  /**
   * Export purchase orders (with line items)
   */
  static async exportPurchaseOrdersCSV(orgId: string, filters?: Filters): Promise<string>;

  /**
   * Upload file to S3
   */
  static async uploadToS3(file: Buffer, filename: string): Promise<string>; // Returns S3 key

  /**
   * Generate presigned URL (24h expiry)
   */
  static async getPresignedURL(s3Key: string): Promise<string>;

  /**
   * Send email notification
   */
  static async sendExportEmail(userId: string, job: ExportJob): Promise<void>;

  /**
   * Estimate row count (before export)
   */
  static async estimateRowCount(orgId: string, entityType: string, filters?: Filters): Promise<number>;

  /**
   * Cleanup expired exports (cron job)
   */
  static async cleanupExpiredExports(): Promise<number>;
}
```

### Frontend Components

```
apps/frontend/app/(authenticated)/integrations/
  export/
    page.tsx                    -- Export configuration page
    jobs/
      page.tsx                  -- Export jobs list
      [id]/
        page.tsx                -- Job status/download page

components/integrations/export/
  ExportConfigForm.tsx          -- Entity, format, filters selector
  EntitySelector.tsx            -- Entity type dropdown
  FormatSelector.tsx            -- CSV/JSON radio buttons
  ExportFilters.tsx             -- Date range, status filters
  ExportJobsList.tsx            -- Jobs DataTable
  ExportJobStatus.tsx           -- Job progress/status widget
  DownloadButton.tsx            -- Download file button
  ReExportButton.tsx            -- Retry export
```

---

## Key Business Rules

1. **Async Threshold**: Exports > 1000 rows processed asynchronously
2. **File Expiry**: Download links expire after 24 hours
3. **Max Export Size**: 100,000 rows per export (configurable)
4. **UTF-8 BOM**: CSV files include BOM for Excel compatibility
5. **Date Format**: YYYY-MM-DD in CSV, ISO 8601 in JSON
6. **Nested Data**: Flattened in CSV, preserved in JSON
7. **RLS Enforcement**: All exports filtered by org_id
8. **Email Notifications**: Sent on completion/failure for async jobs
9. **Retention**: Keep job records for 30 days, files for 24 hours
10. **Permission-Based**: USER can export products/BOMs only, ADMIN all entities

---

## Deliverables

### Database
- [ ] `export_jobs` table with RLS
- [ ] `cleanup_expired_exports()` function
- [ ] Indexes for job queries

### API Routes
- [ ] POST /api/integrations/export
- [ ] GET /api/integrations/export/jobs
- [ ] GET /api/integrations/export/jobs/:id
- [ ] POST /api/integrations/export/jobs/:id/retry

### Service Layer
- [ ] ExportService.createExport()
- [ ] ExportService.processExportJob()
- [ ] ExportService.exportProductsCSV()
- [ ] ExportService.exportProductsJSON()
- [ ] ExportService.uploadToS3()
- [ ] ExportService.getPresignedURL()
- [ ] ExportService.sendExportEmail()
- [ ] Background job processor

### Validation
- [ ] createExportSchema

### Frontend
- [ ] Export config form
- [ ] Jobs list page
- [ ] Job status page
- [ ] Download button
- [ ] Re-export button

### Tests
- [ ] Unit tests: ExportService (>80% coverage)
- [ ] Integration tests: Export endpoints
- [ ] CSV format tests
- [ ] JSON format tests
- [ ] Async job tests
- [ ] E2E: Export workflow

---

## Definition of Done

### Database
- [ ] Table created with constraints
- [ ] RLS enforced
- [ ] Indexes optimized
- [ ] Cleanup function works

### API
- [ ] All endpoints functional
- [ ] Sync exports < 1s for small datasets
- [ ] Async jobs processed correctly
- [ ] Response times acceptable
- [ ] RLS enforced

### Service
- [ ] CSV exports UTF-8 with BOM
- [ ] JSON exports pretty-printed
- [ ] S3 upload works
- [ ] Presigned URLs valid 24h
- [ ] Email notifications sent
- [ ] Background job processes exports

### Frontend
- [ ] Config form validates inputs
- [ ] Jobs list displays correctly
- [ ] Progress updates real-time
- [ ] Download works
- [ ] Re-export functional

### Testing
- [ ] Unit tests: >80% coverage
- [ ] Integration tests passing
- [ ] Format tests verified
- [ ] E2E workflow passing

---

## Risk Assessment

| Risk | Impact | Likelihood | Mitigation |
|------|--------|------------|------------|
| Large exports timeout | MEDIUM | MEDIUM | Async jobs, row limit, pagination |
| S3 storage costs | LOW | MEDIUM | 24h expiry, cleanup job, monitoring |
| Excel encoding issues | MEDIUM | LOW | UTF-8 BOM, test with Excel |
| Memory exhaustion | HIGH | LOW | Stream processing, chunking |

---

## Version History

| Version | Date | Changes | Author |
|---------|------|---------|--------|
| 1.0 | 2026-01-15 | Initial story for Epic 11 Phase 1 | ARCHITECT-AGENT |

---

**Document Status**: Ready for Implementation
**Created**: 2026-01-15
**Lines**: ~575
**Complexity**: M (Medium)
**Phase**: 1 (MVP)
