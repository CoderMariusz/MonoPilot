# 11.11 - Import Templates (Products/BOMs)

**Priority**: P1 (Phase 2)
**Story Points**: M (Medium)
**Type**: fullstack
**Phase**: 2
**Model**: OPUS

**State:** ready
**Estimate:** M (3-4 days)
**Primary PRD:** `docs/1-BASELINE/product/modules/integrations.md` (FR-INT-018, FR-INT-019, Section 4.2)
**Architecture:** CSV parser, validation engine, bulk import with conflict resolution

---

## Goal

Implement bulk import functionality for products and BOMs via Excel/CSV templates. Users download pre-formatted templates, fill with data, upload for validation, preview changes, and execute import with conflict handling (skip/update/ask per row). Reduces manual data entry for initial setup and ongoing updates.

---

## MVP Scope

**MVP Includes**:
- Download Excel/CSV templates (products, BOMs)
- Upload and validate files (Zod schemas)
- Preview import (show first 20 rows with validation status)
- Conflict resolution (skip existing / update existing / ask per row)
- Execute import (create/update products and BOMs)
- Import job queue (async processing for large files)
- Error reporting (downloadable error log)
- Import history log (who imported what, when)

**Deferred to Phase 3**:
- Excel advanced (formulas, multi-sheet)
- Auto-mapping columns (smart detection)
- Import scheduling (cron jobs)
- Rollback failed imports
- Import from other formats (JSON, XML)

---

## User Story

As a **MonoPilot Administrator**, I want to **bulk import products and BOMs via CSV/Excel templates** so that **I can migrate data from existing systems, onboard new products faster, and avoid manual entry errors**.

---

## Scope

**In scope (this story)**
- Product import template (CSV with headers)
- BOM import template (CSV with headers)
- File upload (max 10k rows per file)
- Validation engine (product code unique, allergens valid, etc.)
- Preview import with validation errors highlighted
- Conflict resolution options (skip/update/ask)
- Async import job (Edge Function or background job)
- Error log download (CSV with failed rows + error messages)
- Import history table (audit trail)
- GET /api/integrations/import/products/template (download)
- POST /api/integrations/import/products/validate (validate file)
- POST /api/integrations/import/products/execute (execute import)
- GET /api/integrations/import/jobs/:id/status (check job status)
- Same for BOMs

**Out of scope (this story)**
- Excel advanced features (formulas, macros)
- Multi-sheet import
- Import scheduling
- Rollback mechanism
- JSON/XML imports

---

## Dependencies

### Cross-Epic Dependencies

| Dependency | Story/Epic | Type | What It Provides | Status |
|------------|------------|------|------------------|--------|
| 01.1 | Org Context + RLS | HARD | organizations table | Ready |
| 02.1 | Products CRUD | HARD | products table | Ready |
| 02.2 | BOMs CRUD | HARD | boms, bom_items tables | Ready |

### Within Epic Dependencies

| Dependency | Story | Type | What It Provides |
|------------|-------|------|------------------|
| 11.1 | Integrations Dashboard | HARD | Import section in nav |
| 11.3 | Integration Logs | HARD | Import event logging |

### Provides To (Downstream)

| Story | What This Provides |
|-------|-------------------|
| Phase 3 | Import patterns for other entities (customers, suppliers) |

---

## Database Migration

### Migration: Create import_jobs table

```sql
-- Migration: YYYYMMDDHHMMSS_create_import_jobs.sql

CREATE TABLE import_jobs (
    -- Identity
    id                  UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    org_id              UUID NOT NULL REFERENCES organizations(id) ON DELETE CASCADE,

    -- Job Metadata
    job_type            TEXT NOT NULL, -- 'products', 'boms', 'customers', etc.
    file_name           TEXT NOT NULL,
    file_size           BIGINT NOT NULL, -- bytes
    total_rows          INTEGER NOT NULL,

    -- Processing
    status              TEXT NOT NULL DEFAULT 'pending'
                        CHECK (status IN ('pending', 'validating', 'validated', 'importing', 'completed', 'failed')),
    conflict_resolution TEXT NOT NULL DEFAULT 'skip'
                        CHECK (conflict_resolution IN ('skip', 'update', 'ask')),

    -- Results
    rows_processed      INTEGER DEFAULT 0,
    rows_created        INTEGER DEFAULT 0,
    rows_updated        INTEGER DEFAULT 0,
    rows_skipped        INTEGER DEFAULT 0,
    rows_failed         INTEGER DEFAULT 0,
    validation_errors   JSONB, -- [{row: 5, field: 'product_code', error: 'Already exists'}]

    -- Files
    uploaded_file_url   TEXT, -- Supabase Storage URL
    error_log_url       TEXT, -- CSV with failed rows

    -- Timing
    started_at          TIMESTAMPTZ,
    completed_at        TIMESTAMPTZ,

    -- Audit
    created_at          TIMESTAMPTZ NOT NULL DEFAULT now(),
    created_by          UUID REFERENCES users(id),

    -- Constraints
    CONSTRAINT check_completed CHECK (
        (status = 'completed' AND completed_at IS NOT NULL) OR
        (status != 'completed')
    )
);

-- Indexes
CREATE INDEX idx_import_jobs_org ON import_jobs(org_id);
CREATE INDEX idx_import_jobs_status ON import_jobs(org_id, status);
CREATE INDEX idx_import_jobs_created ON import_jobs(org_id, created_at DESC);
CREATE INDEX idx_import_jobs_type ON import_jobs(org_id, job_type);

-- RLS Policies
ALTER TABLE import_jobs ENABLE ROW LEVEL SECURITY;

CREATE POLICY "import_jobs_select" ON import_jobs
    FOR SELECT USING (
        org_id = (SELECT org_id FROM users WHERE id = auth.uid())
    );

CREATE POLICY "import_jobs_insert" ON import_jobs
    FOR INSERT WITH CHECK (
        org_id = (SELECT org_id FROM users WHERE id = auth.uid())
    );

CREATE POLICY "import_jobs_update" ON import_jobs
    FOR UPDATE USING (
        org_id = (SELECT org_id FROM users WHERE id = auth.uid())
    );
```

---

## Acceptance Criteria (Given/When/Then)

### AC-1: Download Product Import Template

```gherkin
Scenario: Admin downloads product template
  Given admin navigates to /integrations/import/products
  When clicking [Download Template]
  Then CSV file downloaded: product_import_template.csv
  And file contains headers:
    | Column | Description |
    | product_code | Required, unique |
    | name | Required |
    | type | raw_material / semi_finished / finished_good |
    | unit | pcs / kg / L / m |
    | description | Optional |
    | allergens | Comma-separated (e.g., "gluten,milk") |
    | shelf_life_days | Optional |
    | storage_temp | Optional (e.g., "2-8°C") |
    | tax_code | Optional |
  And first row is example data (commented or removable)
```

### AC-2: Upload and Validate Product File

```gherkin
Scenario: Upload valid product file
  Given admin uploads product_import.csv with 100 rows
  And all rows valid (product_code unique, type valid, etc.)
  When clicking [Upload & Validate]
  Then import_jobs record created
  And status = 'validating'
  And backend validates all rows
  And status = 'validated'
  And validation_errors = null
  And success message: "100 rows valid. Ready to import."

Scenario: Upload with validation errors
  Given product_import.csv with 100 rows
  And row 5 has duplicate product_code
  And row 12 has invalid type = 'invalid_type'
  When validating
  Then status = 'validated'
  And validation_errors = [
      {row: 5, field: 'product_code', error: 'Product code PROD-001 already exists'},
      {row: 12, field: 'type', error: 'Invalid type. Must be raw_material, semi_finished, or finished_good'}
    ]
  And preview shows rows 5 and 12 highlighted in red
  And [Fix & Re-upload] button shown
```

### AC-3: Preview Import

```gherkin
Scenario: Preview valid rows
  Given validated import_jobs with 100 rows
  When viewing preview
  Then table shows first 20 rows:
    | Row | Product Code | Name | Type | Status |
    | 1 | PROD-001 | Flour | raw_material | Valid ✓ |
    | 2 | PROD-002 | Sugar | raw_material | Valid ✓ |
    | ... | ... | ... | ... | ... |
  And pagination for remaining rows
  And summary: "100 valid, 0 errors"
  And [Import] button enabled

Scenario: Preview with errors
  Given validated import_jobs with errors
  Then error rows highlighted in red
  And error message shown in tooltip
  And summary: "95 valid, 5 errors"
  And [Import] button disabled
  And [Download Error Log] button enabled
```

### AC-4: Conflict Resolution - Skip

```gherkin
Scenario: Import with conflict_resolution = 'skip'
  Given product_import.csv with 100 rows
  And 10 products already exist with same product_code
  And conflict_resolution = 'skip'
  When executing import
  Then 90 new products created
  And 10 existing products skipped (not updated)
  And rows_created = 90
  And rows_skipped = 10
  And success message: "Import complete. 90 created, 10 skipped."
```

### AC-5: Conflict Resolution - Update

```gherkin
Scenario: Import with conflict_resolution = 'update'
  Given product_import.csv with 100 rows
  And 10 products already exist
  And conflict_resolution = 'update'
  When executing import
  Then 90 new products created
  And 10 existing products updated (name, description, etc. overwritten)
  And rows_created = 90
  And rows_updated = 10
  And success message: "Import complete. 90 created, 10 updated."
```

### AC-6: Conflict Resolution - Ask (Per Row)

```gherkin
Scenario: Import with conflict_resolution = 'ask'
  Given product_import.csv with product_code PROD-001 (already exists)
  And conflict_resolution = 'ask'
  When validating
  Then preview shows:
    | Row | Product Code | Action | Conflict |
    | 5 | PROD-001 | [Skip] [Update] | Existing: "Flour 1kg" -> New: "Flour Premium 1kg" |
  And admin selects action per row
  And import proceeds based on selections
```

### AC-7: Execute Import

```gherkin
Scenario: Execute product import
  Given validated import_jobs with 100 rows
  When clicking [Import]
  Then status = 'importing'
  And progress indicator shown (0/100)
  And backend processes rows asynchronously
  And status updates every 10 rows
  And when complete: status = 'completed'
  And completed_at = now
  And rows_created = 90
  And rows_updated = 10
  And success toast: "Import complete. 90 created, 10 updated."
  And redirect to products list

Scenario: Import job failure
  Given import in progress
  And database error occurs (e.g., connection lost)
  Then status = 'failed'
  And error logged
  And notification sent to admin
  And partial results visible (e.g., 50/100 processed)
```

### AC-8: Download Error Log

```gherkin
Scenario: Download error log for failed rows
  Given import_jobs with validation_errors
  When clicking [Download Error Log]
  Then CSV file downloaded: import_errors_20250115.csv
  And file contains:
    | Row | Product Code | Name | Error |
    | 5 | PROD-001 | Flour | Product code already exists |
    | 12 | PROD-999 | Invalid | Invalid type: 'xyz' |
  And admin can fix errors and re-upload
```

### AC-9: BOM Import Template

```gherkin
Scenario: Download BOM import template
  Given admin navigates to /integrations/import/boms
  When clicking [Download Template]
  Then CSV file downloaded: bom_import_template.csv
  And file contains headers:
    | Column | Description |
    | product_code | Final product code (required) |
    | version | BOM version (required) |
    | item_product_code | Component product code (required) |
    | item_qty | Component quantity (required) |
    | item_unit | Component unit (required) |
    | operation_sequence | Optional |
    | is_output | true/false (default: false) |
```

### AC-10: BOM Import Validation

```gherkin
Scenario: Validate BOM import
  Given bom_import.csv with 50 rows (5 BOMs, 10 items each)
  And all product_codes exist in products table
  And item_qty > 0
  When validating
  Then validation passes
  And preview shows grouped by product_code

Scenario: BOM validation error - product not found
  Given BOM row with product_code = 'INVALID-999'
  And product does not exist
  Then validation_errors = [{row: 5, field: 'product_code', error: 'Product INVALID-999 not found'}]

Scenario: BOM validation error - component not found
  Given BOM row with item_product_code = 'COMP-999'
  And component does not exist
  Then validation_errors = [{row: 5, field: 'item_product_code', error: 'Component COMP-999 not found'}]
```

### AC-11: Import History

```gherkin
Scenario: View import history
  Given admin navigates to /integrations/import/history
  Then DataTable shows import_jobs:
    | Job Type | File Name | Total Rows | Created | Updated | Failed | Status | Date | User |
    | products | products_jan.csv | 100 | 90 | 10 | 0 | completed | 2025-01-15 | admin@example.com |
    | boms | boms_v2.csv | 50 | 45 | 5 | 0 | completed | 2025-01-14 | admin@example.com |
  And filter by job_type, status, date range
  And click to view job details
```

### AC-12: Large File Handling

```gherkin
Scenario: Import large file (10k rows)
  Given product_import.csv with 10,000 rows
  When uploading
  Then file stored in Supabase Storage
  And validation runs asynchronously (Edge Function)
  And progress indicator shows validation status
  And when complete: email notification sent
  And import can be executed

Scenario: File too large
  Given CSV with 50,000 rows
  When uploading
  Then error: "File too large. Maximum 10,000 rows per import."
  And upload blocked
  And suggestion: "Split file into multiple imports"
```

---

## Implementation Notes

### Product Template Columns

```typescript
interface ProductImportRow {
  product_code: string; // Required, unique
  name: string; // Required
  type: 'raw_material' | 'semi_finished' | 'finished_good'; // Required
  unit: string; // Required (pcs, kg, L, m, etc.)
  description?: string;
  allergens?: string; // Comma-separated (gluten,milk,eggs)
  shelf_life_days?: number;
  storage_temp?: string; // e.g., "2-8°C", "Room temp"
  tax_code?: string;
}
```

### BOM Template Columns

```typescript
interface BOMImportRow {
  product_code: string; // Final product
  version: number; // BOM version
  item_product_code: string; // Component
  item_qty: number;
  item_unit: string;
  operation_sequence?: number;
  is_output?: boolean; // By-product flag
}
```

### Validation Rules

```typescript
const productValidationRules = {
  product_code: {
    required: true,
    maxLength: 50,
    unique: true,
  },
  name: {
    required: true,
    maxLength: 255,
  },
  type: {
    required: true,
    enum: ['raw_material', 'semi_finished', 'finished_good'],
  },
  unit: {
    required: true,
    enum: ['pcs', 'kg', 'L', 'm', 'g', 'ml', 'box', 'pallet'],
  },
  allergens: {
    validate: async (value) => {
      if (!value) return true;
      const allergens = value.split(',').map(a => a.trim());
      // Check each allergen exists in allergens table
      return await validateAllergens(allergens);
    },
  },
  shelf_life_days: {
    min: 1,
    max: 3650,
  },
};
```

### Service Layer

```typescript
// lib/services/import-service.ts

export class ImportService {
  /**
   * Generate CSV template for entity type
   */
  static async getTemplate(type: 'products' | 'boms'): Promise<Blob>;

  /**
   * Upload and validate CSV file
   */
  static async uploadAndValidate(
    file: File,
    type: 'products' | 'boms',
    orgId: string,
    userId: string
  ): Promise<ImportJob>;

  /**
   * Execute import job
   */
  static async executeImport(
    jobId: string,
    conflictResolution: 'skip' | 'update',
    orgId: string
  ): Promise<void>;

  /**
   * Get import job status
   */
  static async getJobStatus(jobId: string): Promise<ImportJob>;

  /**
   * Generate error log CSV
   */
  static async getErrorLog(jobId: string): Promise<Blob>;

  /**
   * List import jobs (history)
   */
  static async listJobs(
    orgId: string,
    filters: { type?: string; status?: string; date_from?: string; date_to?: string }
  ): Promise<ImportJob[]>;
}
```

---

## Key Business Rules

1. **Max File Size**: 10,000 rows per import (split larger files)
2. **Validation First**: Must validate before import (no blind imports)
3. **Conflict Resolution**: Default = 'skip' (safe option)
4. **Async Processing**: Files >1k rows processed asynchronously
5. **Error Log**: Always downloadable for failed rows
6. **Audit Trail**: All imports logged to import_jobs table
7. **Rollback (Phase 3)**: MVP does not support rollback, admin must manually fix

---

## Deliverables

### Database
- [ ] Migration: `import_jobs` table
- [ ] RLS policies
- [ ] Indexes

### API Routes
- [ ] `GET /api/integrations/import/products/template`
- [ ] `POST /api/integrations/import/products/validate`
- [ ] `POST /api/integrations/import/products/execute`
- [ ] `GET /api/integrations/import/jobs/:id/status`
- [ ] `GET /api/integrations/import/jobs/:id/error-log`
- [ ] Same for BOMs

### Service Layer
- [ ] `ImportService.getTemplate()`
- [ ] `ImportService.uploadAndValidate()`
- [ ] `ImportService.executeImport()`
- [ ] `ImportService.getJobStatus()`
- [ ] `ImportService.getErrorLog()`

### Frontend
- [ ] Import products page (`/integrations/import/products`)
- [ ] Import BOMs page (`/integrations/import/boms`)
- [ ] Upload wizard (upload -> validate -> preview -> import)
- [ ] Progress indicator
- [ ] Error log viewer
- [ ] Import history page

### Tests
- [ ] Unit tests: Validation rules (>80% coverage)
- [ ] Integration tests: All endpoints
- [ ] E2E: Upload -> Validate -> Import

---

## Definition of Done

### Database
- [ ] Tables created with RLS
- [ ] Indexes created

### API
- [ ] All endpoints functional
- [ ] CSV parsing works
- [ ] Validation works

### Service
- [ ] Template generation
- [ ] Validation engine
- [ ] Import execution

### Frontend
- [ ] Upload wizard works
- [ ] Preview displays correctly
- [ ] Progress indicator functional
- [ ] Error log downloadable

### Testing
- [ ] Unit tests: >80% coverage
- [ ] Integration tests: All endpoints
- [ ] E2E: Full flow passing

---

## Risk Assessment

| Risk | Impact | Likelihood | Mitigation |
|------|--------|------------|------------|
| Large file performance | MEDIUM | MEDIUM | Async processing, 10k row limit |
| Validation complexity | MEDIUM | MEDIUM | Comprehensive Zod schemas, unit tests |
| Partial import failure | MEDIUM | LOW | Transaction per row, error log |

---

## Version History

| Version | Date | Changes | Author |
|---------|------|---------|--------|
| 1.0 | 2026-01-15 | Initial Phase 2 story creation | ARCHITECT |

---

**Document Status**: Ready for Implementation
**Created**: 2026-01-15
**Lines**: ~550
**Complexity**: M (Medium)
**Phase**: 2
