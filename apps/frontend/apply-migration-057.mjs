#!/usr/bin/env node

/**
 * Apply Migration 057: Add warehouse_id to po_header
 * Story 0.1 - Fix PO Header warehouse_id
 */

import { createClient } from '@supabase/supabase-js';
import fs from 'fs';
import path from 'path';
import { fileURLToPath } from 'url';
import dotenv from 'dotenv';

const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);

// Load environment variables
dotenv.config({ path: '.env.local' });

const supabaseUrl = process.env.NEXT_PUBLIC_SUPABASE_URL;
const supabaseServiceKey = process.env.SUPABASE_SERVICE_ROLE_KEY;

if (!supabaseUrl || !supabaseServiceKey) {
  console.error('âŒ Missing required environment variables:');
  console.error('   NEXT_PUBLIC_SUPABASE_URL:', !!supabaseUrl);
  console.error('   SUPABASE_SERVICE_ROLE_KEY:', !!supabaseServiceKey);
  process.exit(1);
}

const supabase = createClient(supabaseUrl, supabaseServiceKey, {
  auth: {
    autoRefreshToken: false,
    persistSession: false
  }
});

async function applyMigration057() {
  try {
    console.log('ğŸš€ Applying Migration 057: Add warehouse_id to po_header\n');

    // Read migration file
    const migrationPath = path.join(__dirname, 'lib', 'supabase', 'migrations', '057_add_warehouse_id_to_po_header.sql');
    const migrationSQL = fs.readFileSync(migrationPath, 'utf8');

    console.log('ğŸ“„ Migration file loaded successfully');
    console.log('ğŸ“ SQL Length:', migrationSQL.length, 'characters');

    // Split SQL into individual statements (excluding comments)
    const statements = migrationSQL
      .split(';')
      .map(s => s.trim())
      .filter(s => s && !s.startsWith('--') && s !== 'END $$');

    console.log('ğŸ“Š Found', statements.length, 'SQL statements\n');

    // Execute each statement
    let successCount = 0;
    for (let i = 0; i < statements.length; i++) {
      const statement = statements[i] + ';';

      // Skip empty or comment-only statements
      if (!statement.trim() || statement.trim() === ';') {
        continue;
      }

      try {
        console.log(`â³ Executing statement ${i + 1}/${statements.length}...`);

        // Use direct SQL execution via Postgres connection
        const { data, error } = await supabase.rpc('exec_sql', {
          sql_query: statement
        });

        if (error) {
          // Try alternative: direct query
          const { error: queryError } = await supabase.from('_sql').select('*').eq('query', statement);

          if (queryError) {
            console.error(`âŒ Error executing statement ${i + 1}:`, error);
            console.error('Statement:', statement.substring(0, 100) + '...');
            continue;
          }
        }

        console.log(`âœ… Statement ${i + 1} executed successfully`);
        successCount++;
      } catch (err) {
        console.error(`âŒ Error on statement ${i + 1}:`, err.message);
        console.error('Statement:', statement.substring(0, 100) + '...');
      }
    }

    console.log(`\nğŸ“Š Migration complete: ${successCount}/${statements.length} statements executed`);

    // Verify migration was applied
    console.log('\nğŸ” Verifying migration...');

    const { data: columns, error: verifyError } = await supabase
      .from('information_schema.columns')
      .select('column_name')
      .eq('table_name', 'po_header')
      .eq('column_name', 'warehouse_id');

    if (verifyError) {
      console.log('âš ï¸  Could not verify migration (RLS may be blocking)');
    } else if (columns && columns.length > 0) {
      console.log('âœ… Migration verified: warehouse_id column exists in po_header');
    } else {
      console.log('âŒ Migration verification failed: warehouse_id column not found');
    }

  } catch (err) {
    console.error('âŒ Error applying migration:', err.message);
    process.exit(1);
  }
}

// Run the migration
applyMigration057()
  .then(() => {
    console.log('\nâœ… Migration 057 applied successfully!');
    process.exit(0);
  })
  .catch((error) => {
    console.error('\nâŒ Migration failed:', error);
    process.exit(1);
  });
